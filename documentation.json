[{"source_path": "documentation/source/changelog.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/changelog.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/changelog.html", "content": "Changelog\n=========\n\n\nv2.2.0 (2024-02-20)\n-------------------\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^\n- feat(llama-index): add embeddings `#(316) <https://github.com/IBM/ibm-generative-ai/pull/316>`_ [`@David-Kristek <https://github.com/David-Kristek>`_]\n\n\nüêõ Bug Fixes\n^^^^^^^^^^^\n- fix: improve http error handling `#(320) <https://github.com/IBM/ibm-generative-ai/pull/320>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix: allow the remaining limit to have a negative value `#(317) <https://github.com/IBM/ibm-generative-ai/pull/317>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix: correct typo in url `#(310) <https://github.com/IBM/ibm-generative-ai/pull/310>`_ [SOTAkkkk]\n\nüìñ Docs\n^^^^^^\n- docs: add simple text generation example `#(323) <https://github.com/IBM/ibm-generative-ai/pull/323>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n\n‚öôÔ∏è Other\n^^^^^^^^\n- chore: fixes and updates `#(318) <https://github.com/IBM/ibm-generative-ai/pull/318>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- typo: fix bam api url in documentation `#(319) <https://github.com/IBM/ibm-generative-ai/pull/319>`_ [Aditya Gupta]\n- docs(langchain): add langchain sql agent example `#(314) <https://github.com/IBM/ibm-generative-ai/pull/314>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- chore: less strict typings `#(315) <https://github.com/IBM/ibm-generative-ai/pull/315>`_ [`@David-Kristek <https://github.com/David-Kristek>`_]\n- chore: improve types generation `#(312) <https://github.com/IBM/ibm-generative-ai/pull/312>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n\n**Full Changelog**: `v2.1.1...v2.2.0 <https://github.com/IBM/ibm-generative-ai/compare/v2.1.1...v2.2.0>`_\n\n\nüîó API Endpoint Versions\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. collapse:: API Endpoint Versions\n\n    ========  ==================================  ======================\n    Method    Path                                Version (YYYY-MM-DD)\n    ========  ==================================  ======================\n    GET       /v2/api_key                         2023-11-22\n    POST      /v2/api_key/regenerate              2023-11-22\n    GET       /v2/files                           2023-12-15\n    POST      /v2/files                           2023-12-15\n    DELETE    /v2/files/{id}                      2023-11-22\n    GET       /v2/files/{id}                      2023-12-15\n    GET       /v2/files/{id}/content              2023-11-22\n    GET       /v2/models                          2023-11-22\n    GET       /v2/models/{id}                     2024-01-30\n    GET       /v2/prompts                         2024-01-10\n    POST      /v2/prompts                         2024-01-10\n    DELETE    /v2/prompts/{id}                    2023-11-22\n    GET       /v2/prompts/{id}                    2024-01-10\n    PATCH     /v2/prompts/{id}                    2024-01-10\n    PUT       /v2/prompts/{id}                    2024-01-10\n    GET       /v2/requests                        2023-11-22\n    DELETE    /v2/requests/chat/{conversationId}  2023-11-22\n    GET       /v2/requests/chat/{conversationId}  2023-11-22\n    DELETE    /v2/requests/{id}                   2023-11-22\n    GET       /v2/system_prompts                  2023-11-22\n    POST      /v2/system_prompts                  2023-11-22\n    DELETE    /v2/system_prompts/{id}             2023-11-22\n    GET       /v2/system_prompts/{id}             2023-11-22\n    PUT       /v2/system_prompts/{id}             2023-11-22\n    GET       /v2/tasks                           2023-11-22\n    POST      /v2/text/chat                       2024-01-10\n    POST      /v2/text/chat/output                2024-01-10\n    POST      /v2/text/chat_stream                2024-01-10\n    POST      /v2/text/embeddings                 2023-11-22\n    GET       /v2/text/embeddings/limits          2023-11-22\n    GET       /v2/text/extraction/limits          2023-11-22\n    POST      /v2/text/generation                 2024-01-10\n    POST      /v2/text/generation/comparison      2023-11-22\n    GET       /v2/text/generation/limits          2023-11-22\n    POST      /v2/text/generation/output          2023-11-22\n    GET       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation/{id}/feedback   2023-11-22\n    PUT       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation_stream          2024-01-10\n    POST      /v2/text/moderations                2023-11-22\n    POST      /v2/text/tokenization               2024-01-10\n    GET       /v2/tunes                           2023-11-22\n    POST      /v2/tunes                           2023-11-22\n    POST      /v2/tunes/import                    2023-11-22\n    DELETE    /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}                      2023-11-22\n    PATCH     /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}/content/{type}       2023-12-15\n    GET       /v2/tuning_types                    2024-01-30\n    DELETE    /v2/user                            2023-11-22\n    GET       /v2/user                            2023-11-22\n    PATCH     /v2/user                            2023-11-22\n    POST      /v2/user                            2023-11-22\n    ========  ==================================  ======================\n\nv2.1.1 (2024-02-02)\n-------------------\n\nüêõ Bug Fixes\n^^^^^^^^^^^\n- fix: make SharedResource threadsafe `#(307) <https://github.com/IBM/ibm-generative-ai/pull/307>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- fix: point readme documentation links to latest version `#(306) <https://github.com/IBM/ibm-generative-ai/pull/306>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n\n‚öôÔ∏è Other\n^^^^^^^^\n- feat(langchain): validate peer dependency `#(308) <https://github.com/IBM/ibm-generative-ai/pull/308>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix(docs): update pre-build hook [`@Tomas2D <https://github.com/Tomas2D>`_]\n\n**Full Changelog**: `v2.1.0...v2.1.1 <https://github.com/IBM/ibm-generative-ai/compare/v2.1.0...v2.1.1>`_\n\n\nv2.1.0 (2024-01-30)\n-------------------\n\n.. admonition:: Schema Import (deprecation warning)\n    :class: warning\n\n    Schemas are now exported from genai.schema (the old way of importing remains to work, but you will receive a warning)\n\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n- feat: refactor schemas for better user experience `#(294) <https://github.com/IBM/ibm-generative-ai/pull/294>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- feat: add truncate_input_tokens parameter for embeddings `#(280) <https://github.com/IBM/ibm-generative-ai/pull/280>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- feat: migrate to langchain_core `#(261) <https://github.com/IBM/ibm-generative-ai/pull/261>`_ [`@David-Kristek <https://github.com/David-Kristek>`_]\n- feat: adjust tests and pipeline to ensure 3.12 compatibility `#(259) <https://github.com/IBM/ibm-generative-ai/pull/259>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- feat: retrieve service actions metadata `#(260) <https://github.com/IBM/ibm-generative-ai/pull/260>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- feat(example): add chromadb embedding function `#(270) <https://github.com/IBM/ibm-generative-ai/pull/270>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- feat(langchain): correctly handles prompt_id and model_id `#(293) <https://github.com/IBM/ibm-generative-ai/pull/293>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- feat(system-prompts): init module `#(292) <https://github.com/IBM/ibm-generative-ai/pull/292>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- feat(langchain): add embeddings support `#(289) <https://github.com/IBM/ibm-generative-ai/pull/289>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- feat(examples): add example of langchain agent with tools `#(268) <https://github.com/IBM/ibm-generative-ai/pull/268>`_ [`@David-Kristek <https://github.com/David-Kristek>`_]\n- feat(langchain): update core and related dependencies `#(282) <https://github.com/IBM/ibm-generative-ai/pull/282>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n\nüêõ Bug Fixes\n^^^^^^^^^^^^^\n- fix: rewrite test casettes due to vcrpy update `#(290) <https://github.com/IBM/ibm-generative-ai/pull/290>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- fix: update vcrpy to released version `#(284) <https://github.com/IBM/ibm-generative-ai/pull/284>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- fix: external limiter implementation `#(274) <https://github.com/IBM/ibm-generative-ai/pull/274>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix: include overhead in payload size calculation when batching `#(266) <https://github.com/IBM/ibm-generative-ai/pull/266>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- fix: reduce maximum payload size [`@jezekra1 <https://github.com/jezekra1>`_]\n- fix: schema action metadata inheritance `#(262) <https://github.com/IBM/ibm-generative-ai/pull/262>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix(docs): redirects `#(298) <https://github.com/IBM/ibm-generative-ai/pull/298>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- fix(langchain): templates and models `#(293) <https://github.com/IBM/ibm-generative-ai/pull/293>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n\nüìñ Docs\n^^^^^^^\n- docs: update links in README [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: update link to the migration guide [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: init documentation versioning `#(296) <https://github.com/IBM/ibm-generative-ai/pull/296>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: add installation note for extensions `#(291) <https://github.com/IBM/ibm-generative-ai/pull/291>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: update prompt usage example `#(275) <https://github.com/IBM/ibm-generative-ai/pull/275>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: update migration guide, examples, deploy `#(271) <https://github.com/IBM/ibm-generative-ai/pull/271>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: update migration guide `#(269) <https://github.com/IBM/ibm-generative-ai/pull/269>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- docs: update README [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: update faq / credentials / migration guide `#(263) <https://github.com/IBM/ibm-generative-ai/pull/263>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- docs: add changelog `#(257) <https://github.com/IBM/ibm-generative-ai/pull/257>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- docs: improve examples `#(258) <https://github.com/IBM/ibm-generative-ai/pull/258>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n\n‚öôÔ∏è Other\n^^^^^^^^\n- build: add langchain to dev dependencies [`@Tomas2D <https://github.com/Tomas2D>`_]\n- refactor: remove list comprehensions to preserve type-hints `#(301) <https://github.com/IBM/ibm-generative-ai/pull/301>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n- ci: update git checkout for documentation build [`@Tomas2D <https://github.com/Tomas2D>`_]\n- ci: update docs build script [`@Tomas2D <https://github.com/Tomas2D>`_]\n- ci: set CODEOWNERS `#(267) <https://github.com/IBM/ibm-generative-ai/pull/267>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- build: remove unused dependencies and update versions `#(264) <https://github.com/IBM/ibm-generative-ai/pull/264>`_ [`@Tomas2D <https://github.com/Tomas2D>`_]\n- ci: check if all tests have markers `#(265) <https://github.com/IBM/ibm-generative-ai/pull/265>`_ [`@jezekra1 <https://github.com/jezekra1>`_]\n\n**Full Changelog**: `v2.0.0...v2.1.0 <https://github.com/IBM/ibm-generative-ai/compare/v2.0.0...v2.1.0>`_\n\n\nüîó API Endpoint Versions\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. collapse:: API Endpoint Versions\n\n    ========  ==================================  ======================\n    Method    Path                                Version (YYYY-MM-DD)\n    ========  ==================================  ======================\n    GET       /v2/api_key                         2023-11-22\n    POST      /v2/api_key/regenerate              2023-11-22\n    GET       /v2/files                           2023-12-15\n    POST      /v2/files                           2023-12-15\n    DELETE    /v2/files/{id}                      2023-11-22\n    GET       /v2/files/{id}                      2023-12-15\n    GET       /v2/files/{id}/content              2023-11-22\n    GET       /v2/models                          2023-11-22\n    GET       /v2/models/{id}                     2024-01-30\n    GET       /v2/prompts                         2024-01-10\n    POST      /v2/prompts                         2024-01-10\n    DELETE    /v2/prompts/{id}                    2023-11-22\n    GET       /v2/prompts/{id}                    2024-01-10\n    PATCH     /v2/prompts/{id}                    2024-01-10\n    PUT       /v2/prompts/{id}                    2024-01-10\n    GET       /v2/requests                        2023-11-22\n    DELETE    /v2/requests/chat/{conversationId}  2023-11-22\n    GET       /v2/requests/chat/{conversationId}  2023-11-22\n    DELETE    /v2/requests/{id}                   2023-11-22\n    GET       /v2/system_prompts                  2023-11-22\n    POST      /v2/system_prompts                  2023-11-22\n    DELETE    /v2/system_prompts/{id}             2023-11-22\n    GET       /v2/system_prompts/{id}             2023-11-22\n    PUT       /v2/system_prompts/{id}             2023-11-22\n    GET       /v2/tasks                           2023-11-22\n    POST      /v2/text/chat                       2024-01-10\n    POST      /v2/text/chat/output                2024-01-10\n    POST      /v2/text/chat_stream                2024-01-10\n    POST      /v2/text/embeddings                 2023-11-22\n    GET       /v2/text/embeddings/limits          2023-11-22\n    GET       /v2/text/extraction/limits          2023-11-22\n    POST      /v2/text/generation                 2024-01-10\n    POST      /v2/text/generation/comparison      2023-11-22\n    GET       /v2/text/generation/limits          2023-11-22\n    POST      /v2/text/generation/output          2023-11-22\n    GET       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation/{id}/feedback   2023-11-22\n    PUT       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation_stream          2024-01-10\n    POST      /v2/text/moderations                2023-11-22\n    POST      /v2/text/tokenization               2024-01-10\n    GET       /v2/tunes                           2023-11-22\n    POST      /v2/tunes                           2023-11-22\n    POST      /v2/tunes/import                    2023-11-22\n    DELETE    /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}                      2023-11-22\n    PATCH     /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}/content/{type}       2023-12-15\n    GET       /v2/tuning_types                    2024-01-30\n    DELETE    /v2/user                            2023-11-22\n    GET       /v2/user                            2023-11-22\n    PATCH     /v2/user                            2023-11-22\n    POST      /v2/user                            2023-11-22\n    ========  ==================================  ======================\n\nv2.0.0 (2024-01-15)\n-------------------\n\nOn November 22nd, 2023, the API (v2) was announced. We reflected this change on the Python SDK by rewriting its core to be faster, more reliable and mainly in sync with the API. The new SDK brings the concept of the central client, which gives you access to the API very straightforward. This concept was recently integrated into OpenAI SDK / Cohere SDK, and more are joining.\n\nTo seamlessly migrate from V0.X versions to 2.0, we have prepared the Migration Guide. The reborn documentation with a lot of examples will help you get started.\n\nHere is a little sneak peek.\n\n\n* Very Performant.\n* Generated Typings directly from the API.\n* Smart Requests Concurrency Handling.\n* Retry Mechanism in case of network or API failure.\n* Batching Large Requests automatically.\n* Easy to extend.\n\n**Full Changelog**: `v0.6.1...v2.0.0 <https://github.com/IBM/ibm-generative-ai/compare/v0.6.1...v2.0.0>`_\n\nüîó API Endpoint Versions\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. collapse:: API Endpoint Versions\n\n    ========  ==================================  ======================\n    Method    Path                                Version (YYYY-MM-DD)\n    ========  ==================================  ======================\n    GET       /v2/api_key                         2023-11-22\n    POST      /v2/api_key/regenerate              2023-11-22\n    GET       /v2/files                           2023-12-15\n    POST      /v2/files                           2023-12-15\n    DELETE    /v2/files/{id}                      2023-11-22\n    GET       /v2/files/{id}                      2023-12-15\n    GET       /v2/files/{id}/content              2023-11-22\n    GET       /v2/models                          2023-11-22\n    GET       /v2/models/{id}                     2024-01-10\n    GET       /v2/prompts                         2024-01-10\n    POST      /v2/prompts                         2024-01-10\n    DELETE    /v2/prompts/{id}                    2023-11-22\n    GET       /v2/prompts/{id}                    2024-01-10\n    PATCH     /v2/prompts/{id}                    2024-01-10\n    PUT       /v2/prompts/{id}                    2024-01-10\n    GET       /v2/requests                        2023-11-22\n    DELETE    /v2/requests/chat/{conversationId}  2023-11-22\n    GET       /v2/requests/chat/{conversationId}  2023-11-22\n    DELETE    /v2/requests/{id}                   2023-11-22\n    GET       /v2/tasks                           2023-11-22\n    POST      /v2/text/chat                       2024-01-10\n    POST      /v2/text/chat/output                2024-01-10\n    POST      /v2/text/chat_stream                2024-01-10\n    POST      /v2/text/embeddings                 2023-11-22\n    GET       /v2/text/embeddings/limits          2023-11-22\n    GET       /v2/text/extraction/limits          2023-11-22\n    POST      /v2/text/generation                 2024-01-10\n    POST      /v2/text/generation/comparison      2023-11-22\n    GET       /v2/text/generation/limits          2023-11-22\n    POST      /v2/text/generation/output          2023-11-22\n    GET       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation/{id}/feedback   2023-11-22\n    PUT       /v2/text/generation/{id}/feedback   2023-11-22\n    POST      /v2/text/generation_stream          2024-01-10\n    POST      /v2/text/moderations                2023-11-22\n    POST      /v2/text/tokenization               2024-01-10\n    GET       /v2/tunes                           2023-11-22\n    POST      /v2/tunes                           2023-11-22\n    POST      /v2/tunes/import                    2023-11-22\n    DELETE    /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}                      2023-11-22\n    PATCH     /v2/tunes/{id}                      2023-11-22\n    GET       /v2/tunes/{id}/content/{type}       2023-12-15\n    GET       /v2/tuning_types                    2023-11-22\n    DELETE    /v2/user                            2023-11-22\n    GET       /v2/user                            2023-11-22\n    PATCH     /v2/user                            2023-11-22\n    POST      /v2/user                            2023-11-22\n    ========  ==================================  ======================\n\nv0.6.1 (2023-12-20)\n-------------------\n\n\n* fix: correct llama-index import for new version by `@David-Kristek <https://github.com/David-Kristek>`_ in `#(243) <https://github.com/IBM/ibm-generative-ai/pull/243>`_\n* fix(examples): correct Hugging Face example prompt by `@David-Kristek <https://github.com/David-Kristek>`_ in `#(244) <https://github.com/IBM/ibm-generative-ai/pull/244>`_\n* fix: prevent duplicating template with same name by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(245) <https://github.com/IBM/ibm-generative-ai/pull/245>`_\n\n**Full Changelog**: `v0.6.0...v0.6.1 <https://github.com/IBM/ibm-generative-ai/compare/v0.6.0...v0.6.1>`_\n\n\nv0.6.0 (2023-12-08)\n-------------------\n\n\n* feat(extensions): add support for llamaindex by `@David-Kristek <https://github.com/David-Kristek>`_ in `#(238) <https://github.com/IBM/ibm-generative-ai/pull/238>`_\n* fix: update aiohttp to support python 3.12 by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(239) <https://github.com/IBM/ibm-generative-ai/pull/239>`_\n* fix: add missing **init**.py in package to fix broken import by `@jezekra1 <https://github.com/jezekra1>`_ in `#(241) <https://github.com/IBM/ibm-generative-ai/pull/241>`_\n* fix: update maximal local concurrency limit based on API response by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(242) <https://github.com/IBM/ibm-generative-ai/pull/242>`_\n\nNew Contributors\n^^^^^^^^^^^^^^^^\n\n\n* `@jezekra1 <https://github.com/jezekra1>`_ made their first contribution in `#(241) <https://github.com/IBM/ibm-generative-ai/pull/241>`_\n\n**Full Changelog**: `v0.5.1...v0.5.2 <https://github.com/IBM/ibm-generative-ai/compare/v0.5.1...v0.5.2>`_\n\n\nv0.5.1 (2023-11-17)\n-------------------\n\nüêõ Bug fixes\n^^^^^^^^^^^^\n\n\n* Add missing rate-limit check for tokenize methods\n* Unify error messages between sync and async methods\n\n**Full Changelog**: `v0.5.0...v0.5.1 <https://github.com/IBM/ibm-generative-ai/compare/v0.5.0...v0.5.1>`_\n\n\nv0.5.0 (2023-11-13)\n-------------------\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Added integration for LangChain Chat Models; see an example of `generation <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/langchain_chat_generate.py>`_ and `streaming <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/langchain_chat_stream.py>`_.\n* Added support for LangChain Model Serialization (saving and loading models); `see an example <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/langchain_serialization.py>`_.\n* Added support for the Chat endpoint in ``Model`` class; see an `example <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/chat.py>`_.\n* Added support for new moderation models (HAP, STIGMA, Implicit Hate) - not released on API yet but will be available soon.\n* Added type validation for input_tokens property in generate response.\n* Extend LangChain generation information / LLM Output (token_usage structure, generated tokens, stop_reason, conversation_id, created_at, ...).\n* Add optional ``raw_response=True/False`` parameter to ``generate_stream`` / ``generate_as_complete`` and ``generate`` methods to receive a raw response instead of unwrapped results.\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* LangChain extension now correctly tokenizes the inputs (previously, the GPT2 tokenizer had been used).\n* Improve general error handling.\n\n**Full Changelog**: `v0.4.1...v0.5.0 <https://github.com/IBM/ibm-generative-ai/compare/v0.4.1...v0.5.0>`_\n\n\nv0.4.1 (2023-10-27)\n-------------------\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* Correctly handle file responses\n* Use ``tqdm.auto`` instead of ``tqdm.tqdm`` to improve display in Jupyter Notebooks\n\n**Full Changelog**: `v0.4.0...v0.4.1 <https://github.com/IBM/ibm-generative-ai/compare/v0.4.0...v0.4.1>`_\n\n\nv0.4.0 (2023-10-24)\n-------------------\n\n‚ö†Ô∏è Switch to Pydantic V2\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* In case your application is dependent on Pydantic V1, refer to the `migration guide <https://docs.pydantic.dev/2.0/migration/>`_.\n* If you cannot upgrade, stick to the previous version 0.3.2.\n\n**Full Changelog**: `v0.3.2...v0.4.0 <https://github.com/IBM/ibm-generative-ai/compare/v0.3.2...v0.4.0>`_\n\n\nv0.3.2 (2023-10-23)\n-------------------\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* Correctly handle async errors and process abortion\n\nüîß Configuration Changes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Increase async generate/tokenize retry limits from 3 to 5\n\n**Full Changelog**: `v0.3.1...v0.3.2 <https://github.com/IBM/ibm-generative-ai/compare/v0.3.1...v0.3.2>`_\n\n\nv0.3.1 (2023-10-20)\n-------------------\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Handle concurrency limits for ``generate`` and ``generate_as_completed`` methods.\n* Add automatic handling of rate limits for the tokenize endpoint (tokenize_async method).\n* Added ``stop_sequence`` parameter for generated output (non-empty token which caused the generation to stop) + added - ``include_stop_sequence`` parameter for the ``GenerateParams`` (it indicates whether the stop sequence (which caused the generation to stop) is part of the generated text. The default value depends on the model in use).\n* Removed hidden ``stop_sequences`` removal inside the ``LangChainInterface``\\ , which can now be controlled via the ``include_stop_sequence`` parameter.\n* Improve general error handling + method signatures (improve Python typings).\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* Fix stacked progress bar (\\ ``generate_async`` method)\n* Handle cases when the package is used inside the ``asyncio`` environment\n* Hide warning when an unknown field is retrieved in the generated response\n\n**Full Changelog**: `v0.3.0...v0.3.1 <https://github.com/IBM/ibm-generative-ai/compare/v0.3.0...v0.3.1>`_\n\n\nv0.3.0 (2023-10-12)\n-------------------\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Added Hugging Face Agent support; see an `example <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/huggingface_agent.py>`_.\n* Drastically improve the speed of ``generate_async`` method - the concurrency limit is now automatically inferred from the API. (custom setting of ``ConnectionManager.MAX_CONCURRENT_GENERATE`` will be ignored). In case you want to slow down the speed of generating, just pass the following parameter to the method: ``max_concurrency_limit=1``  or any other value.\n* Increase the default tokenize processing limits from 5 requests per second to 10 requests per second (this will be increased in the future).\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* Throws on unhandled exceptions during the ``generate_async`` calls.\n  Correctly cleanups the async HTTP clients when the task/calculation is being cancelled (for instance, you call generate_async in Jupyter - Notebook and then click the stop button). This should prevent receiving the ``Can't have two active async_generate_clients`` error.\n* Fix async support for newer LangChain versions (\\ ``>=0.0.300``\\ )\n* Fix LangChain PromptTemplate import warning in newer versions of LangChain\n* Correctly handle server errors when streaming\n* Fix ``tune_methods`` method\n\n\nv0.2.8 (2023-09-25)\n-------------------\n\nüöÄ Features / Enhancements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Added moderation support; now you can retrieve HAP for generated requests (\\ `example <https://github.com/IBM/ibm-generative-ai/blob/main/examples/user/generate_with_moderation.py>`_\\ )\n* Internally improve streaming processing (poor or unstable internet connection)\n* Internally improve server response parsing and error handling\n* Add a user-agent header to distinguish Python SDK on the API\n\nüêõ Bug fixes\n^^^^^^^^^^^^^^^\n\n\n* LangChain - correct handling of stop_sequences\n* Correctly set versions of used dependencies (httpx / pyyaml)\n* Prevents unexpected modifications to user's GenerateParams passed to the Model class\n* Prevents unexpected errors when GenerateParams contains stream=True and generate (non-stream) version is called\n\nüîß Configuration changes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n* Remove API version from the API endpoint string\n\n**Full Changelog**: `v0.2.7...v0.2.8 <https://github.com/IBM/ibm-generative-ai/compare/v0.2.7...v0.2.8>`_\n\n\nv0.2.7 (2023-09-15)\n-------------------\n\n\n* feat(langchain) - generate method by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(157) <https://github.com/IBM/ibm-generative-ai/pull/157>`_\n* fix(params): do not strip special characters by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(153) <https://github.com/IBM/ibm-generative-ai/pull/153>`_\n* fix: correct httpx dependency version by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(158) <https://github.com/IBM/ibm-generative-ai/pull/158>`_\n\n**Full Changelog**: `v0.2.6...v0.2.7 <https://github.com/IBM/ibm-generative-ai/compare/v0.2.6...v0.2.7>`_\n\n\nv0.2.6 (2023-09-11)\n-------------------\n\n\n* feat(langchain): add streaming support by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(144) <https://github.com/IBM/ibm-generative-ai/pull/144>`_\n* feat(http): allow override httpx options by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(149) <https://github.com/IBM/ibm-generative-ai/pull/149>`_\n* feat: add typical_p parameter by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(135) <https://github.com/IBM/ibm-generative-ai/pull/135>`_\n* chore: update examples by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(136) <https://github.com/IBM/ibm-generative-ai/pull/136>`_\n* docs: mention CLI in README by `@Tomas2D <https://github.com/Tomas2D>`_ in `#(143) <https://github.com/IBM/ibm-generative-ai/pull/143>`_\n* chore: adding escapting of backslashes for re.sub value by `@assaftibm <https://github.com/assaftibm>`_ in `#(84) <https://github.com/IBM/ibm-generative-ai/pull/84>`_\n* chore: correct README.md typo by `@ind1go <https://github.com/ind1go>`_ in `#(148) <https://github.com/IBM/ibm-generative-ai/pull/148>`_\n* update schema for stop_sequences generate param by `@mirianfsilva <https://github.com/mirianfsilva>`_ in `#(142) <https://github.com/IBM/ibm-generative-ai/pull/142>`_\n\nNew Contributors\n^^^^^^^^^^^^^^^^\n\n\n* `@assaftibm <https://github.com/assaftibm>`_ made their first contribution in `#(84) <https://github.com/IBM/ibm-generative-ai/pull/84>`_\n* `@ind1go <https://github.com/ind1go>`_ made their first contribution in `#(148) <https://github.com/IBM/ibm-generative-ai/pull/148>`_\n\n**Full Changelog**: `v0.2.5...v0.2.6 <https://github.com/IBM/ibm-generative-ai/compare/v0.2.5...v0.2.6>`_\n\n\nv0.2.5 (2023-08-21)\n-------------------\n\n\n* TOUs handling\n* Update Pydantic version\n* Update examples\n\n**Full Changelog**: `v0.2.4...v0.2.5 <https://github.com/IBM/ibm-generative-ai/compare/v0.2.4...v0.2.5>`_\n\n\nv0.2.4 (2023-08-01)\n-------------------\n\nUpdated the documentation (imports of credentials)\nUpdated schemas for config\nAdded params in GeneratedParams\nUpdated examples\nUpdated tests\n\n\nv0.2.3 (2023-07-24)\n-------------------\n\n\n* Remove ModelType enum\n* Add utils for Model class: listing, info, available, etc.\n* Pydantic model allows extra params\n* Tests\n\n\nv0.2.2 (2023-07-11)\n-------------------\n\nDocumentation Updates.\n\n\nv0.2.1 (2023-07-10)\n-------------------\n\nDocumentation update\nExample update\n\n\nv0.2.0 (2023-07-10)\n-------------------\n\nModel Tuning\nFile manager\nTuning Manager\nModelType deprecation warning\nOpen Source documentation update\n\n\nv0.1.19 (2023-06-30)\n--------------------\n\nFixed pydantic version issue\n\n\nv0.1.18 (2023-06-30)\n--------------------\n\nWatsonx Templating support\nDocumentation and examples' update\nParameters updated for upstream compatibility with sampling method\nRetry mechanism update\n\n\nv0.1.17 (2023-06-23)\n--------------------\n\n\n* Modifications to examples/tests to avoid sampling-related parameters with greedy decoding\n* Updates to build process\n* Modifications to error messages\n\n\nv0.1.16 (2023-06-21)\n--------------------\n\n\n* Documentation update\n* Local server example\n* Open source contributions information\n* Example endpoints updated\n\n\nv0.1.15 (2023-06-08)\n--------------------\n\n\n* üî® GitHub Workflows\n* ‚ú® Progress bar in async_generate function\n* üêõ Updating Terms of Use to use PATCH\n* üé® Adding accessors attribute to model class\n* ‚ú®Search Space example and utils\n* ‚ú® Localserver Extension\n", "type": "documentation"}, {"source_path": "documentation/source/extensions.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/extensions.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/extensions.html", "content": "IBM Generative AI Extensions Guide\n==================================\n\nOpen-source contributors are welcome to extend IBM Gen AI functionality via extensions that must keep the IBM Gen AI package as a dependency. This document explains what extensions are, the types of extensions that exist, and how to create them.\n\n.. contents::\n   :local:\n   :class: this-will-duplicate-information-and-it-is-still-useful-here\n\nIBM Generative AI Core Package\n------------------------------\n\n`IBM Generative AI open-source repository <https://github.com/IBM/ibm-generative-ai>`_ lives under `IBM Github organization <https://github.com/IBM/>`_. This repository is the official and unique location of IBM Gen AI core package source code.\n\nWhat is an IBM Generative AI Extension?\n---------------------------------------\n\nAn extension is a software add-on that is installed on a program to enhance its capabilities. Gen AI extensions are meant to expand the capabilities of the Gen AI core package.\n\nIBM Generative AI Extension types\n---------------------------------\n\nIBM Generative AI extensions can be either of the following:\n\n- official open-source extensions (supported by the project team)\n- third-party open-source extensions\n\nOpen-source \"Gen AI official\" extensions\n----------------------------------------\n\nExtensions are meant for public use from the get-go developed as official open-source extensions.\n\nOwnership and location\n++++++++++++++++++++++\nOpen-source official extensions are typically developed by the Gen AI team, or in collaboration with them. Providing maintenance to open-source official extensions is responsability of the Gen AI team.\n\nOpen-source \"third-party\" extensions\n------------------------------------\n\nAll other extensions neither implemented nor officially maintained by the Gen AI team are referred to as open-source third-party extensions.\n\nOwnership and location\n++++++++++++++++++++++\nMaintaining these extensions would be the responsibility of their owners.\n\nExtensions migration\n--------------------\n\nExtensions might be migrated from one type to another upon evaluation.\n\nOpen-source \"third-party\" to open-source \"Gen AI official\"\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nIf open-source third-party contributors have an interest in their extensions becoming official, they must contact the IBM Gen AI team.\n\nOpen-source \"Gen AI official\" to Gen AI's core\n+++++++++++++++++++++++++++++++++++++++++++++++\n\nUpon evaluation of the Gen AI maintenance team, open-source extensions could be merged with Gen AI core code and become deprecated as extensions.\n\nDesign considerations to build extensions\n-----------------------------------------\n\nAmong other patterns, a developer should consider a mix-and-match of the two following important development experience patterns to design an extension for IBM Gen AI SDK:\n\nExtensions that simulate being part of a core class\n+++++++++++++++++++++++++++++++++++++++++++++++++++\n\nIn spite of the extension's code living in a separate location, **try to give the impression that extended functionality is part of an IBM Gen AI core class**.\n\nExtensions that wrap core class functionality\n+++++++++++++++++++++++++++++++++++++++++++++\n\nDesigning extensions that do not integrate, but **wrap functionality in Gen AI's core** for common use cases. These classes are meant to be used as entities on their own.\n", "type": "documentation"}, {"source_path": "documentation/source/faq.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/faq.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/faq.html", "content": ".. _faq:\n\nFAQ\n===\n\n.. contents::\n   :local:\n   :class: this-will-duplicate-information-and-it-is-still-useful-here\n\n\nSDK ignores SIGINT/SIGTERM signals\n----------------------------------\n\nWe use `asyncio` under the hood, running in its own thread to achieve maximal performance. This means signals from the main\nthread are not propagated; thus, SDK does not know you want to exit. To make SDK reacts to such signal, you need to\ncall our prepared function.\n\nExample can be found :ref:`here <examples.extra.shutdown_handling>`.\n\nWhich API Endpoint should I use?\n--------------------------------\n\nIn previous version, you were used to pass the whole URL including it's path and version - this is not true anymore.\nSee following examples to get it right.\n\n\n.. admonition:: Correct Example\n   :class: success\n\n   ``https://bam-api.res.ibm.com``\n\n\n.. admonition:: Wrong Example\n   :class: error\n\n   ``https://bam-api.res.ibm.com/v1`` (V1 is not supported)\n\n   ``https://bam-api.res.ibm.com/v2`` (V2 is automatically appended)\n\n   ``https://bam-api.res.ibm.com/v2?version=2024-01-01`` (SDK handles API versions internally)\n\n\nWhich endpoints version SDK uses?\n---------------------------------\n\nUse the following options to determine which version of a given endpoint SDK uses.\n\n1. From the :doc:`Changelog <changelog>` (\"API Endpoint Versions\" section) under concrete release.\n\n2. Find it programmatically by retrieving the concrete method's metadata (see :ref:`example <examples.extra.service_metadata>`).\n\n\n\nWhen I run multiple text generations simultaneously one is hanging, how to fix that?\n------------------------------------------------------------------------------------\n\nWhen you run a text generation task, the SDK first retrieves your maximal concurrency limit (this limit can vary from\nuser to user, but generally, there is a default) and use it as the upper bound.\nThis implies that the different processes may get stack or even error.\nTo prevent such a situation, you can set your maximal limit for each execution via the `execution_options` parameter.\n\nSuch an example with multiple limits and processes can be found :ref:`here <examples.extra.parallel_processing>`.\n\n\n.. admonition:: Concurrency limiting without threading/multiprocessing\n   :class: note\n\n   For a single instance of the SDK, concurrency handling is done automatically, but you can still limit the number of requests.\n\n\n.. admonition:: How to find out your current limits?\n   :class: note\n\n   To find your limit, use the following methods ``client.text.generation.limit.retrieve`` (text) / ``client.text.embedding.limit.retrieve`` (embeddings).\n   If your service is not mentioned, try to find the `limit` attribute.\n\n\n.. admonition:: How to increase your limit?\n   :class: note\n\n   Contact administrator or your manager.\n\n\nSSL_CERTIFICATE_VERIFY_FAILED on macOS\n--------------------------------------\n\nThis may be caused by the brew installer for Python not adding the correct certificates.\nPlease create a py file from the following snippet and execute it with ``python3``.\n\n.. code-block:: python\n\n    # install_certifi.py\n    #\n    # sample script to install or update a set of default Root Certificates\n    # for the ssl module.  Uses the certificates provided by the certifi package:\n    #       https://pypi.python.org/pypi/certifi\n\n    import os\n    import os.path\n    import ssl\n    import stat\n    import subprocess\n    import sys\n\n    STAT_0o775 = ( stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR\n                 | stat.S_IRGRP | stat.S_IWGRP | stat.S_IXGRP\n                 | stat.S_IROTH |                stat.S_IXOTH )\n\n\n    def main():\n        openssl_dir, openssl_cafile = os.path.split(\n            ssl.get_default_verify_paths().openssl_cafile)\n\n        print(\" -- pip install --upgrade certifi\")\n        subprocess.check_call([sys.executable,\n            \"-E\", \"-s\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"certifi\"])\n\n        import certifi\n\n        # change working directory to the default SSL directory\n        os.chdir(openssl_dir)\n        relpath_to_certifi_cafile = os.path.relpath(certifi.where())\n        print(\" -- removing any existing file or link\")\n        try:\n            os.remove(openssl_cafile)\n        except FileNotFoundError:\n            pass\n        print(\" -- creating symlink to certifi certificate bundle\")\n        os.symlink(relpath_to_certifi_cafile, openssl_cafile)\n        print(\" -- setting permissions\")\n        os.chmod(openssl_cafile, STAT_0o775)\n        print(\" -- update complete\")\n\n    if __name__ == '__main__':\n        main()\n\n\nSource: https://stackoverflow.com/questions/44649449/brew-installation-of-python-3-6-1-ssl-certificate-verify-failed-certificate/44649450#44649450\n", "type": "documentation"}, {"source_path": "documentation/source/getting_started.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/getting_started.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/getting_started.html", "content": ".. _getting-started:\n\nGetting Started\n===============\n\n.. contents::\n   :local:\n   :class: this-will-duplicate-information-and-it-is-still-useful-here\n\n.. _installation:\n\nInstallation\n------------\n\n.. code-block:: bash\n\n    pip install --upgrade ibm-generative-ai\n\nPrerequisites\n^^^^^^^^^^^^^\n\n- Python version >= 3.9\n\n- pip version >= 22.0.1\n\n.. admonition:: Python 3.12 support\n    :class: warning\n\n    The huggingface extension (:bash:`pip install 'ibm-generative-ai[huggingface]'`) is not supported in python 3.12 yet\n    due to the lack of pytorch support for 3.12.\n    Follow the `pytorch issue <https://github.com/pytorch/pytorch/issues/110436>`_ for more information.\n\n\nCheck your pip version with ``pip --version`` and if needed, run the following command to upgrade pip:\n\n.. code-block:: bash\n\n    pip install --upgrade \"pip>=22.0.1\"\n\n.. _gen-ai-endpoint:\n\nAPI / Credentials\n-------------------\n\nBy default, SDK will use the following API endpoint: ``https://bam-api.res.ibm.com/``. However, if you wish to target a different API, you can do so by defining it with the ``api_endpoint`` argument when you instantiate the ``Credentials`` object.\n\nYour ``.env`` file:\n\n.. code-block:: ini\n\n    GENAI_KEY=YOUR_GENAI_API_KEY\n    GENAI_API=https://bam-api.res.ibm.com\n\n\nExample initialization\n\n.. code-block:: python\n\n    import os\n    from dotenv import load_dotenv\n    from genai.credentials import Credentials\n\n    load_dotenv()\n\n    # if you are using standard ENV variable names (GENAI_KEY / GENAI_API)\n    credentials = Credentials.from_env()\n\n    # or if you are using different ENV variable names\n    credentials = Credentials.from_env(api_key_name=\"MY_ENV_KEY_NAME\", api_endpoint_name=\"MY_ENV_ENDPOINT_NAME\")\n\n    # or if you want to pass values directly\n    credentials = Credentials(api_key=\"MY_API_KEY\", api_endpoint=\"MY_ENDPOINT\")\n\n\nHow to work with SDK?\n---------------------\n\nThe latest version of SDK reflects the latest version of the API (versions are handled automatically).\nThe main ``Client`` class serves as an entry point to the API, while its attributes refer to logically nested services.\nThis approach reflects the Rest API Structure; all request parameters remains the same (SDK does not alter them).\n\n.. code-block:: python\n\n    from genai import Client, Credentials\n\n    credentials = Credentials(api_key=\"...\") # or load from ENV via Credentials.from_env()\n    client = Client(credentials=credentials)\n\n    # client.text (sub-client for all text related tasks)\n\n    # client.text.generation\n    client.text.generation.create(...)\n    client.text.generation.create_stream(...)\n    client.text.generation.limit.retrieve(...)\n\n    # client.text.chat\n    client.text.chat.create(...)\n    client.text.chat.create_stream(...)\n\n    # client.text.embedding\n    client.text.embedding.create(...)\n    client.text.embedding.limit.retrieve(...)\n\n    # client.tokenization\n    client.text.tokenization.create(...)\n\n    # client.moderation\n    client.text.moderation.create(...)\n\n    # client.model\n    client.model.list(...)\n    client.model.retrieve(...)\n\n    # client.tune\n    client.tune.create(...)\n    client.tune.list(...)\n    client.tune.types(...)\n    client.tune.retrieve(...)\n    client.tune.delete(...)\n\n    # client.prompt\n    client.prompt.create(...)\n    client.prompt.list(...)\n    client.prompt.retrieve(...)\n    client.prompt.delete(...)\n    client.prompt.update(...)\n\n    # client.user\n    client.user.create(...)\n    client.user.retrieve(...)\n\n    # client.request\n    client.request.list(...)\n    client.request.chat(...)\n    client.request.delete(...)\n    client.request.chat_delete(...)\n\n    # client.file\n    client.file.list(...)\n    client.file.retrieve(...)\n    client.file.delete(...)\n    client.file.read(...)\n\n\n\nüöÄ To see concrete examples, visit the :doc:`examples page <rst_source/examples>`.\n\nNetworking\n^^^^^^^^^^\n\nBy default, requests time out after 10 minutes (connection timeout is 10 seconds).\nConnection errors and some HTTP status codes are automatically retried.\nThis behaviour can be changed by altering the ``ApiClient`` settings (see examples).\n\n\nVersioning\n^^^^^^^^^^\n\nEach SDK release is only compatible with the latest API version at the time of release. To use the SDK with an older API version, you need to download a version of the SDK tied to the API version you want. Look at the Changelog to see which SDK version to download.\n\n\nTypes / Schemas\n^^^^^^^^^^^^^^^\n\nWast the majority of service methods accepts complex parameters either as instances of appropriate Pydantic class or plain dictionary which is converted to the Pydantic class under the hood.\nAnalogy with enums - you can pass either enum's value or a plain string. Types for inputs/outputs are automatically generated from the OpenAPI definition to Pydantic models.\nResponses are thus automatically validated and provides various built-in helper functions to the user.\n\n\nLogging\n^^^^^^^\n\nSDK uses the standard python `logging module <https://docs.python.org/3/library/logging.html>`_ for logging messages within the module.\nUnless the consuming application explicitly enables logging, no logging messages from GenAI should appear in stdout or stderr e.g. no `print` statements, we should also always log to the `genai` namespace so that logs are easily identifiable.\n\nError Handling\n^^^^^^^^^^^^^^\n\nSDK exception classes (besides Python's built-in) can be imported from ``genai.exceptions``.\n\nValidation errors\n\n- ``ValueError``, ``TypeError``\n- ``ValidationError`` - Pydantic class\n\nAPI / Network errors\n\n- ``ApiNetworkException`` - Unhandled network error (timeout, `httpx` error).\n- ``ApiResponseException`` - Real API response with non 2xx status code.\n\n\nExample can be found :ref:`here <examples.extra.error_handling>`.\n\n\nCitation\n--------\n\nIf this SDK has been significant in your research, and you would like to acknowledge\nthe project in your academic publication, please use the following citation scheme.\n\nBibLaTeX\n^^^^^^^^\n\n.. code-block:: bibtex\n\n    @online{ibm_generative_ai_sdk,\n      author       = {IBM},\n      title        = {IBM Generative AI Python SDK (Tech Preview)},\n      url          = {https://github.com/IBM/ibm-generative-ai},\n      year         = {YYYY},\n      urldate      = {YYYY-MM-DD}\n    }\n\n\nBibTex\n^^^^^^\n\n.. code-block:: bibtex\n\n    @misc{ibm_generative_ai_sdk,\n      author       = {IBM},\n      title        = {IBM Generative AI Python SDK (Tech Preview)},\n      howpublished = {\\url{https://github.com/IBM/ibm-generative-ai}},\n      note         = {Accessed: YYYY-MM-DD},\n      year         = {YYYY}\n    }\n", "type": "documentation"}, {"source_path": "documentation/source/index.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/index.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/index.html", "content": "IBM Generative AI Python SDK (Tech Preview)\n============================================\n\nThis is the Python SDK for IBM Foundation Models Studio to bring IBM Generative AI into Python programs and to also extend it with useful operations and types.\n\nYou can start a trial version or request a demo via https://www.ibm.com/products/watsonx-ai.\n\n.. toctree::\n    :hidden:\n\n    Getting Started <getting_started>\n    V2 Migration Guide <v2_migration_guide>\n    Examples <rst_source/examples>\n    FAQ <faq>\n    Changelog <changelog>\n\n\n.. admonition:: Migration to V2\n   :class: tip\n\n   We recently rewrote nearly the whole SDK to be faster and more reliable and would bring you all the latest features\n   available on the API. To easily migrate, we prepared the :doc:`Migration Guide <v2_migration_guide>`.\n\nTop Features\n------------\n\n‚úÖ Very Performant.\n\n‚úÖ Generated Typings directly from the API.\n\n‚úÖ Smart Requests Concurrency Handling.\n\n‚úÖ Retry Mechanism in case of network or API failure.\n\n‚úÖ Batching Large Requests automatically.\n\n‚úÖ Easy to extend (see :ref:`example <examples.extensions.extra>`).\n\n‚úÖ Integrations to :ref:`LangChain <examples.extensions.langchain>`, :ref:`LLamaIndex <examples.extensions.llama_index>` and :ref:`HuggingFace <examples.extensions.huggingface>`.\n\n‚úÖ :ref:`LocalServer extension <examples.extensions.localserver>` ‚Äì run local API compatible with SDK.\n\n\n\nFirst Steps\n-----------\n\nTo start using the SDK and the power of generative AI, ensure you have an API key from one of the supported environments.\nThen jump straight into :doc:`Getting Started <getting_started>` page and explore all prepared :doc:`examples <rst_source/examples>`.\n\n\nImportant pages\n------------------\n\n* :doc:`Getting Started <getting_started>`\n* :doc:`Examples <rst_source/examples>`\n* :doc:`V2 Migration Guide <v2_migration_guide>`\n* :doc:`Extensions <extensions>`\n* :doc:`FAQ <faq>`\n* :doc:`Modules<rst_source/modules>`\n* :ref:`modindex`\n* :ref:`genindex`\n", "type": "documentation"}, {"source_path": "documentation/source/v2_migration_guide.rst", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/documentation/source/v2_migration_guide.rst", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/v2_migration_guide.html", "content": "V2 Migration Guide\n==================\n\n.. contents::\n   :local:\n   :class: this-will-duplicate-information-and-it-is-still-useful-here\n\nOn November 22nd, 2023, the API (v2) was announced with the following\nchanges.\n\n-  New interface design that can support more modalities (code, image,\n   audio) and tasks (chat, classification, transcription). We have\n   worked on the interfaces together with watsonx.ai and OpenShift AI.\n-  Closer alignment with `watsonx.ai <https://watsonx.ai>`_, so incubating new features in BAM\n   is technically feasible.\n-  Unified experience across REST API, SDKs, CLI and docs.\n-  New SDK design that allows extensibility and reusability across teams\n   and is more intuitive.\n-  Ability to introduce minor breaking changes without affecting users.\n\nThis new design approach lets us rewrite the SDK from scratch and align\nit more towards the new API. In V2, we have introduced the following\nfeatures.\n\n-  Unify methods naming (no more ``generate``,\n   ``generate_as_completed``, ``generate_async`` and so on).\n-  SDK is always up-to date with the latest available API version.\n-  Automatically handles concurrency and rate-limiting for all endpoints\n   without any additional settings. However, one can explicitly set a\n   custom concurrency limit (generate / embeddings) or batch size\n   (tokenization).\n-  Add implementation for every endpoint that exists on the API (generation limits, generation feedback, prompts, moderations, ‚Ä¶).\n-  Improve overall speed by re-using HTTP clients, improving concurrency\n   handling and utilising API processing power.\n-  Automatically generate request/output pydantic data models (data\n   models are always up to date).\n-  BAM API is a new default environment.\n\nWhat has changed?\n-----------------\n\n- All responses that used to contain the ``results`` field of object type have gotten the field renamed to ``result``.\n- ``totalCount`` param in paginated responses is renamed to ``total_count``.\n- Most methods return the whole response instead of some of its subfield.\n- When you see in examples dedicated classes for parameters like ``TextGenerationParameters``, you can always pass a dictionary which will be converted to the class under the hood; same applies to the enums.\n- Errors are raised immediately instead of being swallowed (can be changed via an appropriate parameter).\n- The ``Credentials`` class throws when an invalid endpoint is passed.\n- The majority of schemas were renamed (``GenerateParams`` -> ``TextGenerationParameters``, ‚Ä¶); for instance, if you work with text generation service (``client.text.generation.create``), all schemas can be found in ``genai.features.text.generation``, this analogy applies to every other service.\n- ``tqdm`` package has been removed as we think it should not be part of the core layer. One can easily use it by wrapping the given SDK function (see :ref:`example <examples.text.generation>`).\n- ``Model`` class has been replaced with a more general ``Client``, an entry point for all services.\n- ``Options`` class has been removed, as every parameter is unpacked at the method level.\n\n\nText Generation\n---------------\n\n\nReplacing ``generate``/``generate_as_completed``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Credentials, Model\n   from genai.schemas import GenerateParams\n\n   credentials = Credentials.from_env()\n   parameters = GenerateParams(max_new_tokens=10)\n\n   model = Model(\"google/flan-ul2\", params=parameters, credentials=credentials)\n   results = model.generate([\"What is IBM?\"]) # or model.generate_as_completed([\"What is IBM?\"])\n   print(f\"Generated Text: {results[0].generated_text}\")\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Credentials, Client\n   from genai.schema import TextGenerationParameters, TextGenerationReturnOptions\n\n   credentials = Credentials.from_env()\n   parameters = TextGenerationParameters(max_new_tokens=10)\n\n   client = Client(credentials=credentials)\n   responses = list(\n       client.text.generation.create(\n           model_id=\"google/flan-ul2\",\n           inputs=[\"What is IBM?\"],\n           parameters=parameters,\n           # optionally request more details in the output:\n           return_options=TextGenerationReturnOptions(generated_tokens=True, token_logprobs=True)\n       )\n   )\n   print(f\"Generated Text: {responses[0].results[0].generated_text}\")\n\nYou can see that the newer way is more typing, but you can retrieve\ntop-level information like: ``id``, ``created_at``, ‚Ä¶\n\nüìù Notes\n\n- Old ``generate`` method returns the list of generated responses whereas the new ``create`` method returns a generator\n\nüëâ See more :ref:`Text Generation Examples <examples.text>`.\n\n\nReplacing `stream` parameter\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Credentials, Model\n   from genai.schemas import GenerateParams\n\n   credentials = Credentials.from_env()\n   parameters = GenerateParams(streaming=True, max_new_tokens=30)\n\n   model = Model(\"google/flan-ul2\", params=parameters, credentials=credentials)\n   for response in model.generate([\"What is IBM?\"], raw_response=True):\n       print(response)\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Credentials, Client\n   from genai.schema import TextGenerationParameters\n\n   credentials = Credentials.from_env()\n   parameters = TextGenerationParameters(max_new_tokens=30)\n\n   client = Client(credentials=credentials)\n   for response in client.text.generation.create_stream(model_id=\"google/flan-ul2\", input=\"What is IBM?\"):\n       print(response)\n\nüìù Notes\n\n- ``stream`` parameter has been removed; use ``create_stream`` method instead.\n\nüëâ See more complex :ref:`Text Generation Streaming Example <examples.text.generation_streaming>`.\n\n\nReplacing ``generate_async``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe old ``generate_async`` method has worked by sending multiple requests asynchronously (it spawns a new thread and runs an event loop). This is now a default behaviour for the ``create`` method in ``GenerationService`` (``client.text.generation.create``).\n\n.. code:: python\n\n   from tqdm.auto import tqdm\n   from genai import Client, Credentials\n\n   credentials = Credentials.from_env()\n   client = Client(credentials=credentials)\n   prompts = [\"Prompt A\", \"Prompt B\", \"...\"]\n\n   for response in tqdm(\n       total=len(prompts),\n       desc=\"Progress\",\n       unit=\" inputs\",\n       iterable=client.text.generation.create(\n           model_id=\"google/flan-ul2\",\n           inputs=prompts\n       )\n   ):\n       print(f\"Response ID: {response.id}\")\n       print(response.results)\n\nüìù Notes\n\n-  ``max_concurrency_limit``/``callback`` parameters are now located\n   under ``execution_options`` parameter.\n\n-  ``options`` parameter has been removed; every possible request\n   parameter is now being parameter of the function; for instance: in\n   previous version ``prompt_id`` had to be part of ``options``\n   parameter, now ``prompt_id`` is a standalone function parameter.\n\n-  results are now automatically in-order (``ordered=True``), old\n   behaviour was ``ordered=False``/\n\n-  ``throw_on_error`` is by default set to ``True`` (old behaviour -\n   set to ``False`` by default). In case of ``True``, you will never\n   receive a ``None`` as a response.\n\n-  ``return_raw_response`` parameter was removed, the raw response is\n   now returned automatically (this is why you need to write\n   ``response.results[0].generated_text`` instead of\n   ``response.generated_text``; although it may seem more complex it‚Äôs\n   more robust because you will never lose any information contained at\n   the top-level).\n\n-  ``tqdm`` progressbar together with ``hide_progressbar`` property has\n   been removed; you now have to use ``tqdm`` in your own (see example\n   above).\n\nüëâ See more complex :ref:`Text Generation Example <examples.text.generation>`.\n\nTokenization\n------------\n\nSimilarly to ``generation`` related unification; ``tokenization``\nservice provides a single ``create`` method, which does the heavy lifting\nfor you. With the new API, we have decided to remove constraints on the input\nitems length; however, HTTP payload size and rate limiting are still\nthere and new SDK takes care of it by ensuring that input items are\ndynamically chunked based on their byte size and by user-provided limit\n(if provided). So it‚Äôs up to you if you have any limitations on the input\nsize.\n\n\nReplacing ``tokenize`` / ``tokenize_as_completed`` / ``tokenize_async``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Credentials, Model\n   from genai.schemas import GenerateParams\n\n   credentials = Credentials.from_env()\n   model = Model(\"google/flan-ul2\", params=GenerateParams(max_new_tokens=20), credentials=credentials)\n   prompts = [\"What is IBM?\"] * 100\n\n   for response in model.tokenize_async(prompts, return_tokens=True, ordered=True):\n       print(response.results)\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Client, Credentials\n   from genai.schema import TextTokenizationParameters, TextTokenizationReturnOptions\n   from genai.text.tokenization import CreateExecutionOptions\n\n   credentials = Credentials.from_env()\n   client = Client(credentials=credentials)\n   prompts = [\"What is IBM?\"] * 100\n\n   for response in client.text.tokenization.create(\n       model_id=\"google/flan-ul2\",\n       input=prompts,\n       parameters=TextTokenizationParameters(\n          return_options=TextTokenizationReturnOptions(\n                tokens=True,  # return tokens\n          )\n       ),\n       execution_options=CreateExecutionOptions(\n          ordered=True,\n          batch_size=5,  # (optional) every HTTP request will contain maximally requests,\n          concurrency_limit=10,  # (optional) maximally 10 requests wil run at the same time\n       ),\n   ):\n       print(response.results)\n\nüìù Notes\n\n-  ``results`` are now ordered by default\n-  ``throw_on_error`` is by default set to ``True`` (old behaviour - set to ``False`` by default).In case of ``True``, you will never receive a ``None`` as a response.\n-  ``return_tokens``/``callbacks`` parameter is now located under ``parameters``.\n-  ``client.text.tokenization.create`` returns a ``generator`` instead of ``list``, to work with it as a list, just do ``responses = list(client.text.tokenization.create(...))``.\n-  ``stop_reason`` enums are changing from ``SCREAMING_SNAKE_CASE`` to ``snake_case`` (e.g. ``MAX_TOKENS`` -> ``max_tokens``), you can use the prepared ``StopReason`` enum.\n\nüëâ See :ref:`Text Tokenization Example <examples.text.tokenization>`.\n\nModels\n------\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Model, Credentials\n\n   credentials = Credentials.from_env()\n   all_models = Model.list(credentials=credentials)\n\n   model = Model(\"google/flan-ul2\", credentials=credentials)\n   detail = model.info() # get info about current model\n   is_available = model.available() # check if model exists\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Client, Credentials\n\n   credentials = Credentials.from_env()\n   client = Client(credentials=credentials)\n\n   all_models = client.model.list(offset=0, limit=100) # parameters are optional\n   detail = client.model.retrieve(\"google/flan-ul2\")\n   is_available = True # model exists otherwise previous line would throw an exception\n\nüìù Notes\n\n-  Client throws an exception when a model does not exist instead of returning ``None``.\n-  Client always returns the whole response instead of the response results.\n-  Pagination has been added.\n\nüëâ See :ref:`Model Example <examples.model.model>`.\n\n\nFiles\n-----\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Model, Credentials\n   from genai.services import FileManager\n   from genai.schemas import FileListParams\n\n   credentials = Credentials.from_env()\n\n   file_list = FileManager.list_files(credentials=credentials, params=FileListParams(offset=0, limit=5))\n   file_metadata = FileManager.file_metadata(credentials=credentials, file_id=\"id\")\n   file_content = FileManager.read_file(credentials=credentials, file_id=\"id\")\n   uploaded_file = FileManager.upload_file(credentials=credentials, file_path=\"path_on_your_system\", purpose=\"tune\")\n   FileManager.delete_file(credentials=credentials, file_id=\"id\")\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Client, Credentials\n   from genai.schema import FilePurpose\n\n   credentials = Credentials.from_env()\n   client = Client(credentials=credentials)\n\n   file_list = client.file.list(offset=0, limit=5) # you can pass way more filters\n   file_metadata = client.file.retrieve(\"id\")\n   file_content = client.file.read(\"id\")\n   uploaded_file = client.file.create(file_path=\"path_on_your_system\", purpose=FilePurpose.TUNE) # or just purpose=\"tune\"\n   client.file.delete(credentials=credentials, file_id=\"id\")\n\n\nüëâ See :ref:`Files Example <examples.file.file>`.\n\n\nTunes\n-----\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai import Model, Credentials\n   from genai.services import TuneManager\n   from genai.schemas.tunes_params import (\n       CreateTuneHyperParams,\n       CreateTuneParams,\n       DownloadAssetsParams,\n       TunesListParams,\n   )\n\n   credentials = Credentials.from_env()\n\n   tune_list = TuneManager.list_tunes(credentials=credentials, params=TunesListParams(offset=0, limit=5))\n   tune_methods = TuneManager.get_tune_methods(credentials=credentials)\n   tune_detail = TuneManager.get_tune(credentials=credentials, tune_id=\"id\")\n   tune_content = TuneManager.download_tune_assets(credentials=credentials, params=DownloadAssetsParams(id=\"tune_id\", content=\"encoder\"))\n   upload_tune = TuneManager.create_tune(credentials=credentials, params=CreateTuneParams(model_id=\"google/flan-ul2\", task_id=\"generation\", name=\"my tuned model\", method_id=\"pt\", parameters=CreateTuneHyperParams(...)))\n   TuneManager.delete_tune(credentials=credentials, tune_id=\"id\")\n\n   # or via `Model` class\n\n   model =  Model(\"google/flan-ul2\", params=None, credentials=credentials)\n   tuned_model = model.tune(\n       name=\"my tuned model\",\n       method=\"pt\",\n       task=\"generation\",\n       hyperparameters=CreateTuneHyperParams(...)\n   )\n   tuned_model.download(...)\n   tuned_model.info(...)\n   tuned_model.delete(...)\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Client, Credentials\n   from genai.schema import TuneStatus, TuningType, TuneAssetType\n\n   credentials = Credentials.from_env()\n   client = Client(credentials=credentials)\n\n   tune_list = client.tune.list(offset=0, limit=5, status=TuneStatus.COMPLETED) # or just status=\"completed\"\n   tune_methods = client.tune.types()\n   tune_detail = client.tune.retrieve(\"tune_id\")\n   tune_content = client.tune.read(id=\"tune_id\", type=TuneAssetType.LOGS) # or type=\"logs\"\n   upload_tune = client.tune.create(name=\"my tuned model\", model_id=\"google/flan-ul2\", task_id=\"generation\", tuning_type=TuningType.PROMPT_TUNING) # tuning_type=\"prompt_tuning\"\n   client.tune.delete(\"tune_id\")\n\nüìù Notes\n\n- ``task`` is now ``task_id``\n- ``method_id`` is now ``tuning_type``, the list of allowable values has changed (use ``TuningType`` enum or values from the documentation; accepted values are changing from ``pt`` and ``mpt`` to ``prompt_tuning`` and ``multitask_prompt_tuning``).\n- ``init_method`` enums are changing from ``SCREAMING_SNAKE_CASE`` to ``snake_case`` (e.g. ``RANDOM`` -> ``random``)\n- ``status`` enums are changing from ``SCREAMING_SNAKE_CASE`` to ``snake_case`` (e.g. ``COMPLETED`` -> ``completed``), you can use the prepared ``TuneStatus`` enum.\n\nüëâ See :ref:`Tune a Custom Model Example <examples.tune.tune>`.\n\n\nPrompt Template (Prompt Pattern)\n--------------------------------\n\nThe ``PromptPattern`` class has been removed as it was a local\nduplication of the API‚Äôs Prompt Templates (Prompts). Prompt Templates\nhave been replaced by the more general ``Prompts``.\n\nSee the following example if you want to create a reusable prompt\n(prompt with a template).\n\n.. code:: python\n\n   from genai import Client, Credentials\n\n   client = Client(credentials=Credentials.from_env())\n\n   # Create prompt\n   prompt_response = client.prompt.create(\n       model_id=\"google/flan-ul2\",\n       name=\"greet prompt\",\n       input=\"Hello {{name}}, enjoy your flight to {{destination}}!\",\n       data={\"name\": \"Mr./Mrs.\", \"destination\": \"Unknown\"}, # optional\n   )\n   prompt_id = prompt_response.result.id\n\n   # Render prompt via text generation endpoint\n   generate_response = client.text.generation.create(\n       prompt_id=prompt_id,\n       data={\n           \"name\": \"Alex\",\n           \"destination\": \"London\"\n       }\n   )\n\n   # Response: Hello Alex, enjoy your flight to London!\n   print(f\"Response: {next(generate_response).results[0].generated_text}\")\n\n\nüëâ See :ref:`Custom prompt with variables Example <examples.prompt.prompt>`.\n\n\nHistory (Requests History)\n--------------------------\n\n‚ùå Old approach\n\n.. code:: python\n\n   from genai.credentials import Credentials\n   from genai.metadata import Metadata\n   from genai.schemas.history_params import HistoryParams\n\n\n   metadata = Metadata(Credentials.from_env())\n   params = HistoryParams(\n       limit=8,\n       offset=0,\n       status=\"SUCCESS\",\n       origin=\"API\",\n   )\n\n   history_response = metadata.get_history(params)\n\n‚úÖ New approach\n\n.. code:: python\n\n   from genai import Client, Credentials\n   from genai.request import RequestStatus, RequestRetrieveOriginParameter\n\n   client = Client(credentials=Credentials.from_env())\n\n   history_response = client.request.list(\n       limit=8,\n       offset=0,\n       status=RequestStatus.SUCCESS,  # or status=\"success\"\n       origin=RequestRetrieveOriginParameter.API,  # or origin=\"api\"\n   )\n\nüìù Notes\n\n- ``status``, ``origin`` and endpoint ``enums`` are changing from ``SCREAMING_SNAKE_CASE`` to ``snake_case`` (e.g. ``SUCCESS`` -> ``success``). Feel free to use prepared Python enums.\n- By default, all origins are now returned (as opposed to generate only in v1).\n- Response object now includes ``version`` field describing major and minor version of API used when the request was created.\n- Requests made under v1 as well as v2 are returned (while v1/requests endpoint returns only v1 requests).\n\nüëâ See :ref:`Requests (History) Example <examples.request.request>`.\n\nExtensions\n----------\n\nüìù Notes\n\n- ``PandasExtension`` was removed, because the functionality was replaced by API's prompt templates.\n- The ``params`` class attribute has been renamed to `parameters` (everywhere).\n- The ``model`` class attribute has been renamed to `model_id` (everywhere).\n- Third party extensions were updated to work with latest versions of the libraries.\n- If you were using local models through a ``LocalLLMServer``, you may need to adjust them to the new parameter and return types.\n\nüëâ See :ref:`All Extensions Examples <examples.extensions>`.\n", "type": "documentation"}, {"source_path": "examples/extensions/huggingface/huggingface_agent.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/huggingface/huggingface_agent.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.huggingface.huggingface_agent.html", "content": "\"\"\"\nRun Transformers Agents\n\n.. admonition:: Python 3.12 support\n    :class: warning\n\n    The huggingface extension (:bash:`pip install 'ibm-generative-ai[huggingface]'`) is not supported in python 3.12 yet\n    due to the lack of pytorch support for 3.12.\n    Follow the `pytorch issue <https://github.com/pytorch/pytorch/issues/110436>`_ for more information.\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client\nfrom genai.credentials import Credentials\nfrom genai.extensions.huggingface.agent import IBMGenAIAgent\nfrom genai.schema import TextGenerationParameters\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nload_dotenv()\n\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"Transformers Agent\"))\n\n\nagent = IBMGenAIAgent(\n    client=client,\n    model=\"meta-llama/llama-2-70b-chat\",\n    parameters=TextGenerationParameters(min_new_tokens=10, max_new_tokens=200, random_seed=777, temperature=0),\n)\n\nagent.chat(\"Extract text from the given url\", url=\"https://research.ibm.com/blog/analog-ai-chip-low-power\")\nagent.chat(\"Do the text summarization on the downloaded text.\")\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_agent.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_agent.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_agent.html", "content": "\"\"\"LangChain agent\n\nThe agent chooses a sequence of actions to respond to a human's question. It has access to a set of tools.\nThe agent memorizes the conversation history and can use it to make decisions.\n\"\"\"\n\nfrom typing import Optional\n\nfrom dotenv import load_dotenv\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.format_scratchpad import format_log_to_str\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.tools.render import render_text_description_and_args\nfrom langchain_core.callbacks import CallbackManagerForToolRun\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.tools import BaseTool\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainChatInterface\nfrom genai.schema import TextGenerationParameters\n\nload_dotenv()\n\n\nclass WordLengthTool(BaseTool):\n    name = \"GetWordLength\"\n    description = \"Returns the length of a word.\"\n\n    def _run(self, word: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> int:\n        return len(word)\n\n\ntools: list[BaseTool] = [WordLengthTool()]\n\nsystem_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n{tools}\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\nValid \"action\" values: \"Final Answer\" or {tool_names}\nProvide only ONE action per $JSON_BLOB, as shown:\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n```\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n```\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"\nhuman_prompt = \"\"\"{input}\n{agent_scratchpad}\n(reminder to respond in a JSON blob no matter what)\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\", optional=True),\n        (\"human\", human_prompt),\n    ]\n)\n\nclient = Client(credentials=Credentials.from_env())\nllm = LangChainChatInterface(\n    client=client,\n    model_id=\"meta-llama/llama-2-70b-chat\",\n    parameters=TextGenerationParameters(\n        max_new_tokens=250, min_new_tokens=20, temperature=0, stop_sequences=[\"\\nObservation\"]\n    ),\n)\nprompt = prompt.partial(\n    # get tools with their descriptions and args in plain text\n    tools=render_text_description_and_args(list(tools)),\n    tool_names=\", \".join([t.name for t in tools]),\n)\n\nmemory = ConversationBufferMemory()\n\nagent = (\n    RunnablePassthrough.assign(\n        # format the agent's scratchpad to a string\n        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n        # pass the memory as the chat history\n        chat_history=lambda x: memory.chat_memory.messages,\n    )\n    | prompt\n    | llm\n    | JSONAgentOutputParser()\n)\nagent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)\n\nagent_executor.invoke({\"input\": \"How many letters are in the word educa?\"})\nagent_executor.invoke({\"input\": \"That's not a real word, can you tell me a valid word?\"})\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_chat_generate.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_chat_generate.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_chat_generate.html", "content": "\"\"\"Chat with a model using LangChain\"\"\"\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain.chat_llm import LangChainChatInterface\nfrom genai.schema import (\n    DecodingMethod,\n    ModerationHAP,\n    ModerationParameters,\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nllm = LangChainChatInterface(\n    client=Client(credentials=Credentials.from_env()),\n    model_id=\"meta-llama/llama-2-70b-chat\",\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=100,\n        min_new_tokens=10,\n        temperature=0.5,\n        top_k=50,\n        top_p=1,\n        return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n    ),\n    moderations=ModerationParameters(\n        # Threshold is set to very low level to flag everything (testing purposes)\n        # or set to True to enable HAP with default settings\n        hap=ModerationHAP(input=True, output=False, threshold=0.01)\n    ),\n)\n\nprint(heading(\"Start conversation with langchain\"))\nprompt = \"Describe what is Python in one sentence.\"\nprint(f\"Request: {prompt}\")\nresult = llm.generate(\n    messages=[\n        [\n            SystemMessage(\n                content=\"\"\"You are a helpful, respectful and honest assistant.\nAlways answer as helpfully as possible, while being safe.\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\nPlease ensure that your responses are socially unbiased and positive in nature. If a question does not make\nany sense, or is not factually coherent, explain why instead of answering something incorrectly.\nIf you don't know the answer to a question, please don't share false information.\n\"\"\",\n            ),\n            HumanMessage(content=prompt),\n        ]\n    ],\n)\nconversation_id = result.generations[0][0].generation_info[\"meta\"][\"conversation_id\"]\nprint(f\"New conversation with ID '{conversation_id}' has been created!\")\nprint(f\"Response: {result.generations[0][0].text}\")\nprint(result.llm_output)\nprint(result.generations[0][0].generation_info)\n\nprint(heading(\"Continue conversation with langchain\"))\nprompt = \"Show me some simple code example.\"\nprint(f\"Request: {prompt}\")\nresult = llm.generate(\n    messages=[[HumanMessage(content=prompt)]], conversation_id=conversation_id, use_conversation_parameters=True\n)\nprint(f\"Response: {result.generations[0][0].text}\")\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_chat_stream.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_chat_stream.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_chat_stream.html", "content": "\"\"\"Streaming response from LangChain\"\"\"\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainChatInterface\nfrom genai.schema import DecodingMethod, TextGenerationParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"Stream chat with langchain\"))\n\nllm = LangChainChatInterface(\n    model_id=\"meta-llama/llama-2-70b-chat\",\n    client=Client(credentials=Credentials.from_env()),\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=100,\n        min_new_tokens=10,\n        temperature=0.5,\n        top_k=50,\n        top_p=1,\n    ),\n)\n\nprompt = \"Describe what is Python in one sentence.\"\nprint(f\"Request: {prompt}\")\nfor chunk in llm.stream(\n    input=[\n        SystemMessage(\n            content=\"\"\"You are a helpful, respectful and honest assistant.\nAlways answer as helpfully as possible, while being safe.\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\nPlease ensure that your responses are socially unbiased and positive in nature. If a question does not make\nany sense, or is not factually coherent, explain why instead of answering something incorrectly.\nIf you don't know the answer to a question, please don't share false information.\n\"\"\",\n        ),\n        HumanMessage(content=prompt),\n    ],\n):\n    print(f\"Chunk Received:\\n  Token: '{chunk.content}'\\n  Info:{chunk.generation_info}\")\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_embeddings.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_embeddings.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_embeddings.html", "content": "\"\"\"\nLangChain Embeddings\n\"\"\"\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainEmbeddingsInterface\nfrom genai.schema import TextEmbeddingParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"LangChain Embeddings\"))\n\nclient = Client(credentials=Credentials.from_env())\nembeddings = LangChainEmbeddingsInterface(\n    client=client,\n    model_id=\"sentence-transformers/all-minilm-l6-v2\",\n    parameters=TextEmbeddingParameters(truncate_input_tokens=True),\n)\n\nquery_embedding = embeddings.embed_query(\"Hello world!\")\nprint(query_embedding)\n\ndocuments_embedding = embeddings.embed_documents([\"First document\", \"Second document\"])\nprint(documents_embedding)\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_generate.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_generate.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_generate.html", "content": "\"\"\"Text generation using LangChain\"\"\"\n\nfrom typing import Any, Optional\nfrom uuid import UUID\n\nfrom dotenv import load_dotenv\nfrom langchain_core.callbacks.base import BaseCallbackHandler\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainInterface\nfrom genai.schema import (\n    DecodingMethod,\n    ModerationHAP,\n    ModerationParameters,\n    TextGenerationParameters,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"Generate text with langchain\"))\n\n\nclass Callback(BaseCallbackHandler):\n    def on_llm_new_token(\n        self,\n        token: str,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        print(f\"Token received: {token}\")\n\n\nllm = LangChainInterface(\n    model_id=\"google/flan-t5-xl\",\n    client=Client(credentials=Credentials.from_env()),\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=10,\n        min_new_tokens=1,\n        temperature=0.5,\n        top_k=50,\n        top_p=1,\n    ),\n    moderations=ModerationParameters(\n        # Threshold is set to very low level to flag everything (testing purposes)\n        # or set to True to enable HAP with default settings\n        hap=ModerationHAP(input=True, output=True, threshold=0.01)\n    ),\n)\n\nprompt = \"Tell me about IBM.\"\nprint(f\"Prompt: {prompt}\")\n\nresult = llm.generate(prompts=[prompt], callbacks=[Callback()])\n\nprint(f\"Answer: {result.generations[0][0].text}\")\nprint(result.llm_output)\nprint(result.generations[0][0].generation_info)\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_generate_with_template.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_generate_with_template.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_generate_with_template.html", "content": "\"\"\"Use LangChain generation with a custom template.\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainInterface\nfrom genai.schema import TextGenerationParameters, TextGenerationReturnOptions\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprompt_response = client.prompt.create(\n    name=\"Recipe Generator Prompt\",\n    model_id=\"google/flan-t5-xl\",\n    input=\"Make a short recipe for {{meal}} (use bullet points)\",\n)\n\ntry:\n    llm = LangChainInterface(\n        client=client,\n        model_id=\"ibm/granite-13b-instruct-v2\",\n        prompt_id=prompt_response.result.id,\n        parameters=TextGenerationParameters(\n            min_new_tokens=100,\n            max_new_tokens=500,\n            return_options=TextGenerationReturnOptions(input_text=False, input_tokens=True),\n        ),\n        data={\"meal\": \"Lasagne\"},\n    )\n    for chunk in llm.stream(\"\"):\n        print(chunk, end=\"\")\nfinally:\n    # Delete the prompt if you don't need it\n    client.prompt.delete(prompt_response.result.id)\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_qa.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_qa.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_qa.html", "content": "\"\"\"QA using native LangChain features\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainInterface\nfrom genai.schema import DecodingMethod, TextGenerationParameters\n\ntry:\n    from langchain_core.output_parsers import StrOutputParser\n    from langchain_core.prompts import PromptTemplate\nexcept ImportError:\n    raise ImportError(\"Could not import langchain: Please install ibm-generative-ai[langchain] extension.\")\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"QA with Langchain\"))\n\nparameters = TextGenerationParameters(\n    decoding_method=DecodingMethod.SAMPLE,\n    max_new_tokens=100,\n    min_new_tokens=1,\n    temperature=0.5,\n    top_k=50,\n    top_p=1,\n)\n\npt1 = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Generate a random question about {topic}: Question: \",\n)\npt2 = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Answer the following question: {question}\",\n)\n\n\nclient = Client(credentials=Credentials.from_env())\nmodel_id = \"google/flan-ul2\"\nflan = LangChainInterface(model_id=model_id, client=client, parameters=parameters)\nmodel = LangChainInterface(model_id=model_id, client=client)\n\nprompt_to_flan_chain = pt1 | flan | StrOutputParser()\nflan_to_model_chain = pt2 | model | StrOutputParser()\n\nchain = {\"question\": prompt_to_flan_chain} | flan_to_model_chain\nprint(chain.invoke({\"topic\": \"life\"}))\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_serialization.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_serialization.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_serialization.html", "content": "\"\"\"Serialize LangChain model to a file\"\"\"\n\nimport tempfile\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainInterface\nfrom genai.schema import (\n    DecodingMethod,\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"Serialize a langchain chain\"))\n\nllm = LangChainInterface(\n    model_id=\"google/flan-ul2\",\n    client=client,\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=10,\n        min_new_tokens=1,\n        temperature=0.5,\n        top_k=50,\n        top_p=1,\n        return_options=TextGenerationReturnOptions(generated_tokens=True, token_logprobs=True, input_tokens=True),\n    ),\n)\n\nwith tempfile.NamedTemporaryFile(suffix=\".json\") as tmp:\n    print(f\"Serializing LLM instance into '{tmp.name}'\")\n    llm.save(tmp.name)\n    print(f\"Loading serialized instance from '{tmp.name}'\")\n    llm_new = LangChainInterface.load_from_file(file=tmp.name, client=client)\n    print(\"Comparing old instance with the new instance\")\n    assert llm == llm_new\n    print(f\"Done, removing '{tmp.name}'\")\n", "type": "example"}, {"source_path": "examples/extensions/langchain/langchain_sql_agent.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/langchain/langchain_sql_agent.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.langchain.langchain_sql_agent.html", "content": "\"\"\"LangChain SQL Agent\n\nIn this example, we first create an SQL database with a ‚Äòcountries‚Äô table, and subsequently, we will use LangChain\nAgent to make queries against it.\n\"\"\"\n\nimport contextlib\nfrom tempfile import TemporaryFile\n\nfrom dotenv import load_dotenv\nfrom langchain.agents import AgentExecutor\nfrom langchain.agents.format_scratchpad import format_log_to_str\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.tools.render import render_text_description_and_args\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.tools import BaseTool\n\nfrom genai import Client, Credentials\nfrom genai.extensions.langchain import LangChainChatInterface\nfrom genai.schema import TextGenerationParameters\n\nload_dotenv()\n\ntry:\n    import pandas as pd\n    import sqlalchemy\n    from langchain.sql_database import SQLDatabase\n    from langchain_community.agent_toolkits import SQLDatabaseToolkit\nexcept ImportError:\n    print(\"Please install 'pandas' / 'sqlalchemy' to run this example.\")\n    raise\n\n\n@contextlib.contextmanager\ndef get_countries_db():\n    with TemporaryFile(suffix=\".db\") as f:\n        df = pd.DataFrame(\n            {\n                \"country\": [\n                    \"United States\",\n                    \"United Kingdom\",\n                    \"France\",\n                    \"Germany\",\n                    \"Italy\",\n                    \"Spain\",\n                    \"Canada\",\n                    \"Australia\",\n                    \"Japan\",\n                    \"China\",\n                ],\n                \"gdp\": [\n                    19294482071552,\n                    2891615567872,\n                    2411255037952,\n                    3435817336832,\n                    1745433788416,\n                    1181205135360,\n                    1607402389504,\n                    1490967855104,\n                    4380756541440,\n                    14631844184064,\n                ],\n            }\n        )\n\n        engine = sqlalchemy.create_engine(f\"sqlite:///{f.name}\")\n        df.to_sql(\"countries\", con=engine, index=False, if_exists=\"replace\")\n        yield SQLDatabase.from_uri(f\"sqlite:///{f.name}\")\n        engine.dispose(close=True)\n\n\ndef create_llm():\n    client = Client(credentials=Credentials.from_env())\n    return LangChainChatInterface(\n        client=client,\n        model_id=\"meta-llama/llama-2-70b-chat\",\n        parameters=TextGenerationParameters(\n            max_new_tokens=250, min_new_tokens=20, temperature=0, stop_sequences=[\"\\nObservation\"]\n        ),\n    )\n\n\ndef create_agent(tools: list[BaseTool], llm: LangChainChatInterface):\n    system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n    {tools}\n    Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n    Valid \"action\" values: \"Final Answer\" or {tool_names}\n    Provide only ONE action per $JSON_BLOB, as shown:\n    ```\n    {{\n      \"action\": $TOOL_NAME,\n      \"action_input\": $INPUT\n    }}\n    ```\n    Follow this format:\n    Question: input question to answer\n    Thought: consider previous and subsequent steps\n    Action:\n    ```\n    $JSON_BLOB\n    ```\n    Observation: action result\n    ... (repeat Thought/Action/Observation N times)\n    Thought: I know what to respond\n    Action:\n    ```\n    {{\n      \"action\": \"Final Answer\",\n      \"action_input\": \"Final response to human\"\n    }}\n    Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n    Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"  # noqa\n\n    human_prompt = \"\"\"{input}\n    {agent_scratchpad}\n    (reminder to respond in a JSON blob no matter what)\"\"\"\n\n    memory = ConversationBufferMemory()\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(\"chat_history\", optional=True),\n            (\"human\", human_prompt),\n        ]\n    ).partial(\n        tools=render_text_description_and_args(list(tools)),\n        tool_names=\", \".join([t.name for t in tools]),\n    )\n\n    agent = (\n        RunnablePassthrough.assign(\n            # format the agent's scratchpad to a string\n            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n            # pass the memory as the chat history\n            chat_history=lambda x: memory.chat_memory.messages,\n        )\n        | prompt\n        | llm\n        | JSONAgentOutputParser()\n    )\n\n    return AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory)\n\n\nwith get_countries_db() as db:\n    llm = create_llm()\n\n    sql_toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n    tools = sql_toolkit.get_tools()\n\n    agent_executor = create_agent(tools, llm)\n    agent_executor.invoke({\"input\": \"How many rows are in the countries table?\"})\n    agent_executor.invoke({\"input\": \"Which are the countries with GDP greater than 3000000000000?\"})\n", "type": "example"}, {"source_path": "examples/extensions/llama_index/llama_index_embedding.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/llama_index/llama_index_embedding.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.llama_index.llama_index_embedding.html", "content": "\"\"\"\nLlamaIndex Embeddings\n\"\"\"\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.extensions.llama_index import IBMGenAILlamaIndexEmbedding\nfrom genai.schema import TextEmbeddingParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"LlamaIndex Embeddings\"))\n\nclient = Client(credentials=Credentials.from_env())\nembeddings = IBMGenAILlamaIndexEmbedding(\n    client=client,\n    model_id=\"sentence-transformers/all-minilm-l6-v2\",\n    parameters=TextEmbeddingParameters(truncate_input_tokens=True),\n)\n\nquery_embedding = embeddings.get_query_embedding(\"Hello world!\")\nprint(query_embedding)\n\ndocuments_embedding = embeddings.get_agg_embedding_from_queries([\"First document\", \"Second document\"])\nprint(documents_embedding)\n", "type": "example"}, {"source_path": "examples/extensions/llama_index/llama_index_llm.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/llama_index/llama_index_llm.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.llama_index.llama_index_llm.html", "content": "\"\"\"Use a model through LLamaIndex\"\"\"\n\nfrom dotenv import load_dotenv\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\nfrom genai import Client\nfrom genai.credentials import Credentials\nfrom genai.extensions.llama_index import IBMGenAILlamaIndex\nfrom genai.schema import DecodingMethod, TextGenerationParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nllm = IBMGenAILlamaIndex(\n    client=client,\n    model_id=\"meta-llama/llama-2-70b-chat\",\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=100,\n        min_new_tokens=10,\n        temperature=0.5,\n        top_k=50,\n        top_p=1,\n    ),\n)\n\nprint(heading(\"Complete text with llama index\"))\nprompt = \"What is a molecule?\"\nprint(f\"Prompt: {prompt}\")\nresult = llm.complete(prompt)\nprint(f\"Answer: {result}\")\n\nprint(heading(\"Chat with llama index\"))\nprompt = \"Describe what is Python in one sentence.\"\nprint(f\"Prompt: {prompt}\")\nmessage = llm.chat(\n    messages=[\n        ChatMessage(\n            role=MessageRole.SYSTEM,\n            content=\"You are a helpful, respectful and honest assistant.\",\n        ),\n        ChatMessage(role=MessageRole.USER, content=prompt),\n    ],\n)\nprint(f'Answer: \"{message}\"')\n", "type": "example"}, {"source_path": "examples/extensions/localserver/local_client.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/localserver/local_client.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.localserver.local_client.html", "content": "\"\"\"Customize behavior of local client\"\"\"\n\nfrom typing import Generator\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.client import BaseServices as ClientServices\nfrom genai.schema import TextGenerationCreateResponse\nfrom genai.text.generation.generation_service import GenerationService\nfrom genai.text.text_service import TextService\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclass LocalGenerationService(GenerationService):\n    def create(self, **kwargs) -> Generator[TextGenerationCreateResponse, None, None]:\n        for response in super().create(**kwargs):\n            response.results[0].generated_text = \"You've been hacked!\"\n            yield response\n\n\nclass LocalTextService(TextService):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs, services=TextService.Services(GenerationService=LocalGenerationService))\n\n\nclass LocalClient(Client):\n    def __init__(self, credentials: Credentials):\n        super().__init__(credentials=credentials, services=ClientServices(TextService=LocalTextService))\n\n\nprint(heading(\"Use custom text service implementation\"))\n\n# Instantiate a custom client\nclient = LocalClient(credentials=Credentials.from_env())\nfor response in client.text.generation.create(model_id=\"google/flan-t5-xl\", inputs=\"aha!\"):\n    for result in response.results:\n        print(result.generated_text)\n", "type": "example"}, {"source_path": "examples/extensions/localserver/local_server.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extensions/localserver/local_server.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extensions.localserver.local_server.html", "content": "\"\"\"\nUse a local server with a custom model\n\n.. admonition:: Python 3.12 support\n    :class: warning\n\n    The transformers library is not supported in python 3.12 yet due to the lack of pytorch support for 3.12.\n    Follow the `pytorch issue <https://github.com/pytorch/pytorch/issues/110436>`_ for more information.\n\"\"\"\n\nimport logging\n\nfrom genai.client import Client\n\n# Import the ibm-generative-ai library and local server extension\nfrom genai.extensions.localserver import LocalLLMServer, LocalModel\nfrom genai.schema import (\n    DecodingMethod,\n    StopReason,\n    TextGenerationParameters,\n    TextGenerationResult,\n    TextGenerationReturnOptions,\n    TextTokenizationCreateResults,\n    TextTokenizationParameters,\n)\n\n# This example uses the transformers library, please install using:\n# pip install transformers torch sentencepiece\ntry:\n    from transformers import T5ForConditionalGeneration, T5Tokenizer\nexcept ImportError:\n    raise ImportError(\n        \"\"\"\nCould not import transformers which is needed for this example.\nPlease install using: pip install transformers torch sentencepiece\n\"\"\"\n    )\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass FlanT5Model(LocalModel):\n    model_id = \"google/flan-t5-base\"\n\n    def __init__(self):\n        logger.info(\"Initialising my custom flan-t5-base model\")\n        self.tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n        logger.info(\"flan-t5-base is ready!\")\n\n    def generate(self, input_text: str, parameters: TextGenerationParameters) -> TextGenerationResult:\n        logger.info(f\"Calling generate on: {input_text}\")\n        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n        response = self.model.generate(input_ids, max_new_tokens=parameters.max_new_tokens)\n\n        genai_response = TextGenerationResult(\n            generated_text=self.tokenizer.decode(response[0]),\n            generated_token_count=response.shape[1],\n            input_token_count=input_ids.shape[1],\n            stop_reason=StopReason.EOS_TOKEN,\n            input_text=input_text if parameters.return_options.input_text else None,\n        )\n        logger.info(f\"Response to {input_text} was: {genai_response}\")\n\n        return genai_response\n\n    def tokenize(self, input_text: str, parameters: TextTokenizationParameters) -> TextTokenizationCreateResults:\n        logger.info(f\"Calling tokenize on: {input_text}\")\n        tokenized = self.tokenizer(input_text).input_ids\n        tokens = self.tokenizer.convert_ids_to_tokens(tokenized)\n        return TextTokenizationCreateResults(\n            token_count=len(tokens),\n            tokens=tokens if parameters.return_tokens else None,\n        )\n\n\nprint(heading(\"Use a local server with a custom model\"))\n\n# Instantiate the Local Server with your model\nserver = LocalLLMServer(models=[FlanT5Model])\n\n# Start the server and execute your code\nwith server.run_locally():\n    print(\" > Server is started\")\n    # Instantiate a custom client\n    client = Client(credentials=server.get_credentials())\n\n    # Instantiate parameters for chat\n    parameters = TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=10,\n        return_options=TextGenerationReturnOptions(input_text=True),\n    )\n\n    prompts = [\"Hello! How are you?\", \"How's the weather?\"]\n    for response in client.text.generation.create(model_id=FlanT5Model.model_id, inputs=prompts, parameters=parameters):\n        result = response.results[0]\n        print(f\"Prompt: {result.input_text}\\nResponse: {result.generated_text}\")\n\n\nprint(\" > Server stopped, goodbye!\")\n", "type": "example"}, {"source_path": "examples/extra/custom_api_client.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/custom_api_client.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.custom_api_client.html", "content": "\"\"\"\nCustomize underlying API (httpx) Client\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai import ApiClient, Client, Credentials\nfrom genai.schema import DecodingMethod, TextGenerationParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"Alter HTTP communication by modifying API Client\"))\n\n\ncredentials = Credentials.from_env()\n\napi_client = ApiClient(\n    credentials=credentials,\n    config={\n        # Allow up to 2 retries, disable SSL verification\n        \"transport_options\": {\"retries\": 2, \"verify\": False},\n        # Disable SSL verification, set general timeout to 10 seconds with specific timeout for establishing connection\n        \"client_options\": {\"verify\": False, \"timeout\": {\"timeout\": 10, \"connect\": 3}},\n    },\n)\nclient = Client(api_client=api_client)\n\n\nresponses = list(\n    client.text.generation.create(\n        model_id=\"google/flan-ul2\",\n        inputs=[\"Generate a random number\"],\n        parameters=TextGenerationParameters(decoding_method=DecodingMethod.SAMPLE, max_new_tokens=5, min_new_tokens=1),\n    )\n)\nprint(f\"Your random number is: {responses[0].results[0].generated_text}\")\n", "type": "example"}, {"source_path": "examples/extra/error_handling.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/error_handling.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.error_handling.html", "content": "\"\"\"\nError Handling\n\nHow to handle SDK exceptions.\n\"\"\"\nfrom genai import Client, Credentials\nfrom genai.exceptions import ApiNetworkException, ApiResponseException, ValidationError\nfrom genai.schema import TextGenerationParameters\n\ncredentials = Credentials.from_env()\nclient = Client(credentials=credentials)\n\ntry:\n    response = client.text.generation.create(\n        model_id=\"non-existing-model\",\n        inputs=[\"Hello world!\"],\n        parameters=TextGenerationParameters(temperature=1, max_new_tokens=50),\n    )\n    print(response)\nexcept ApiResponseException as e:\n    print(e.message)  # our handcrafted message\n    print(e.response.model_dump_json())  # parsed error response from the API\n    # {\n    #     \"status_code\": 404,\n    #     \"error\": \"Not Found\",\n    #     \"message\": \"Model not found\",\n    #     \"extensions\": {\n    #         \"code\": \"NOT_FOUND\",\n    #         \"state\": {\n    #             \"model_id\": \"non-existing-model\"\n    #         }\n    #     }\n    # }\nexcept ApiNetworkException as e:\n    print(\"The server could not be reached\")\n    print(e.__cause__)  # an underlying exception from 'httpx' library\nexcept ValidationError as e:\n    print(\"Provided parameters are not valid\")\n    print(e)\n", "type": "example"}, {"source_path": "examples/extra/logging_example.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/logging_example.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.logging_example.html", "content": "\"\"\"\nEnable/Disable logging for SDK\n\nFollow the official logging documentation for more options:  https://docs.python.org/3/library/logging.config.html\n\"\"\"\n\nimport logging\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\nlogging.basicConfig(level=logging.INFO)\n\n# Most GEN-ai logs are at Debug level, so you can specifically enable\n# or change the genai logging level here\nlogging.getLogger(\"genai\").setLevel(logging.DEBUG)\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"Generate text with detailed logs\"))\n\nclient = Client(credentials=Credentials.from_env())\nprompts = [\"Hello! How are you?\", \"How's the weather?\"]\nfor response in client.text.generation.create(\n    model_id=\"google/flan-ul2\",\n    inputs=prompts,\n    parameters=TextGenerationParameters(\n        decoding_method=DecodingMethod.SAMPLE,\n        max_new_tokens=10,\n        return_options=TextGenerationReturnOptions(input_text=True),\n    ),\n):\n    result = response.results[0]\n    print(f\"Prompt: {result.input_text}\\nResponse: {result.generated_text}\")\n", "type": "example"}, {"source_path": "examples/extra/parallel_processing.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/parallel_processing.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.parallel_processing.html", "content": "\"\"\"\nText generation with custom concurrency limit and multiple processes\n\nThe following example depicts how to limit concurrency to be able to process outputs from multiple models at once.\nSuch a technique is necessary to prevent spending all resources in the first instance.\n\"\"\"\nfrom multiprocessing import Pool\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\ndef run(model_id: str, limit: int):\n    from dotenv import load_dotenv\n\n    from genai import Client, Credentials\n    from genai.schema import TextGenerationParameters\n    from genai.text.generation import CreateExecutionOptions\n\n    # make sure you have a .env file under genai root with\n    # GENAI_KEY=<your-genai-key>\n    # GENAI_API=<genai-api-endpoint>\n    load_dotenv()\n\n    client = Client(credentials=Credentials.from_env())\n\n    for response in client.text.generation.create(\n        model_id=model_id,\n        inputs=[\"What is a molecule?\"] * 20,\n        execution_options=CreateExecutionOptions(concurrency_limit=limit, ordered=False),\n        parameters=TextGenerationParameters(min_new_tokens=10, max_new_tokens=25),\n    ):\n        print(f\"[{model_id}] Generated Text: {response.results[0].generated_text}\")\n\n\nif __name__ == \"__main__\":\n    parameters = [(\"google/flan-ul2\", 3), (\"google/flan-t5-xl\", 5), (\"google/flan-t5-xxl\", 2)]\n    print(heading(f\"Run text generation in {len(parameters)} processes\"))\n\n    with Pool(processes=len(parameters)) as pool:\n        pool.starmap(run, parameters)\n", "type": "example"}, {"source_path": "examples/extra/service_metadata.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/service_metadata.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.service_metadata.html", "content": "\"\"\"\nRetrieve metadata for given service method\n\nFrom metadat you can see for instance which endpoint does the method uses.\n\"\"\"\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.utils import get_service_action_metadata\n\nload_dotenv()\n\ncredentials = Credentials.from_env()\nclient = Client(credentials=credentials)\n\nmetadata = get_service_action_metadata(client.text.generation.create)\nprint(metadata.model_dump())\n", "type": "example"}, {"source_path": "examples/extra/service_overriding.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/service_overriding.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.service_overriding.html", "content": "\"\"\"\nOverriding built-in services\n\nIf you want to override or extend the behaviour of some service, you can do so by providing your own class.\nGenerally, you can achieve it by supplying your service on the local level (preferred) or global level.\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai import ApiClient, Client, Credentials\nfrom genai.text import TextService\nfrom genai.text.generation import GenerationService as OriginalGenerationService\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef local_approach(api_client: ApiClient):\n    \"\"\"Locally override services (preferred)\"\"\"\n\n    class MyGenerationService(OriginalGenerationService):\n        def my_custom_method(self):\n            return \"Greeting!\"\n\n    class TextServiceServices(TextService.Services):\n        GenerationService: type[OriginalGenerationService] = MyGenerationService\n\n    class MyTextService(TextService):\n        Services = TextServiceServices\n\n    client = Client(api_client=api_client, services=Client.Services(TextService=MyTextService))\n    client.text.generation.my_custom_method()\n\n\ndef global_approach(api_client: ApiClient):\n    \"\"\"Globally override services (non preferred)\"\"\"\n\n    class MyGenerationService(OriginalGenerationService):\n        def my_custom_method(self):\n            pass\n\n    class MyTextService(TextService.Services):\n        GenerationService: type[OriginalGenerationService] = MyGenerationService\n\n    TextService.Services = MyTextService\n\n    client = Client(api_client=api_client)\n    client.text.generation.my_custom_method()\n\n\ncredentials = Credentials.from_env()\napi_client = ApiClient(credentials=credentials)\n\nlocal_approach(api_client)\nglobal_approach(api_client)\n", "type": "example"}, {"source_path": "examples/extra/shutdown_handling.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/shutdown_handling.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.shutdown_handling.html", "content": "\"\"\"\nShutdown Handling\n\nTo drastically improve SDKS's performance, we spawn an additional thread with its event pool (asyncio).\nBut because the non-main thread cannot be listening to signals like SIGINT/SIGTERM, we need the owner of the\nmain thread to do so; this is why we provide the 'handle_shutdown_event' function, which allows you to signalise\nthe termination action to the SDK.\n\"\"\"\nimport signal\n\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials, handle_shutdown_event\nfrom genai.schema import TextGenerationParameters\n\nload_dotenv()\n\n# If you add comments to the following lines, it will prevent the application from being canceled smoothly.\n# This is because the SDK will delay the termination process until completion.\n# However, by using the signal functionality, the application can be stopped almost instantly.\nsignal.signal(signal.SIGINT, handle_shutdown_event)\nsignal.signal(signal.SIGTERM, handle_shutdown_event)\n\ncredentials = Credentials.from_env()\nclient = Client(credentials=credentials)\n\ntry:\n    responses = list(\n        client.text.generation.create(\n            model_id=\"google/flan-t5-xl\",\n            inputs=[\"Summarize human evolution from it's beginning.\"] * 50,\n            parameters=TextGenerationParameters(min_new_tokens=100, max_new_tokens=250),\n        )\n    )\nexcept InterruptedError:\n    print(\"Generation has been aborted.\")\n", "type": "example"}, {"source_path": "examples/extra/vector_database/chroma_db/chroma_db_embedding.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/extra/vector_database/chroma_db/chroma_db_embedding.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.extra.vector_database.chroma_db.chroma_db_embedding.html", "content": "\"\"\"\nCreate ChromaDB Embedding Function\n\"\"\"\nfrom typing import Optional\n\nfrom chromadb.api.types import Documents, EmbeddingFunction, Embeddings\nfrom dotenv import load_dotenv\n\nfrom genai import Client, Credentials\nfrom genai.schema import TextEmbeddingParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\nclass ChromaEmbeddingFunction(EmbeddingFunction):\n    def __init__(self, *, model_id: str, client: Client, parameters: Optional[TextEmbeddingParameters] = None):\n        self._model_id = model_id\n        self._parameters = parameters\n        self._client = client\n\n    def __call__(self, inputs: Documents) -> Embeddings:\n        embeddings: Embeddings = []\n        for response in self._client.text.embedding.create(\n            model_id=self._model_id, inputs=inputs, parameters=self._parameters\n        ):\n            embeddings.extend(response.results)\n\n        return embeddings\n\n\ncredentials = Credentials.from_env()\nclient = Client(credentials=credentials)\nembedding_fn = ChromaEmbeddingFunction(model_id=\"sentence-transformers/all-minilm-l6-v2\", client=client)\n\nprint(embedding_fn([\"Hello world!\"]))\n", "type": "example"}, {"source_path": "examples/file/file.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/file/file.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.file.file.html", "content": "\"\"\"\nWorking with files\n\nThe following example shows how to create/retrieve/read/delete a file from API.\n\"\"\"\n\nimport os\nimport tempfile\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client, Credentials\nfrom genai.schema import FileListSortBy, FilePurpose, SortDirection\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\nclient = Client(credentials=Credentials.from_env())\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nwith tempfile.NamedTemporaryFile(delete=True, suffix=\".json\") as tmp:\n    content = '{\"input\": \"<input>\", \"output\": \"<ideal output>\"}'\n    tmp.write(content.encode())\n    tmp.seek(0)\n\n    filename = os.path.basename(tmp.name)\n\n    print(heading(\"Upload file\"))\n    upload_result = client.file.create(file_path=tmp.name, purpose=FilePurpose.TUNE)\n    file_id = upload_result.result.id\n    print(f\"File ID: {file_id}\")\n\n    try:\n        print(heading(\"List files\"))\n        for file in client.file.list(\n            limit=5,\n            search=filename,\n            sort_by=FileListSortBy.CREATED_AT,\n            direction=SortDirection.DESC,\n            purpose=FilePurpose.TUNE,\n        ).results:\n            pprint(file.model_dump())\n\n        print(heading(\"Get file metadata\"))\n        metadata_result = client.file.retrieve(file_id).result\n        pprint(metadata_result.model_dump())\n\n        print(heading(\"Get file content\"))\n        print(client.file.read(file_id))\n    finally:\n        print(heading(\"Delete file\"))\n        client.file.delete(file_id)\n        print(\"OK\")\n", "type": "example"}, {"source_path": "examples/model/model.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/model/model.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.model.model.html", "content": "\"\"\"\nShow information about supported models\n\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\nclient = Client(credentials=Credentials.from_env())\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"List all models\"))\nfor model in client.model.list(limit=100).results:\n    print(model.model_dump(include=[\"name\", \"id\"]))\n\nprint(heading(\"Get model detail\"))\nmodel_detail = client.model.retrieve(\"google/flan-t5-xl\").result\npprint(model_detail.model_dump(include=[\"name\", \"description\", \"id\", \"developer\", \"size\"]))\n", "type": "example"}, {"source_path": "examples/prompt/prompt.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/prompt/prompt.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.prompt.prompt.html", "content": "\"\"\"\nCreate a custom prompt with variables\n\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import DecodingMethod, LengthPenalty, TextGenerationParameters, TextGenerationReturnOptions\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\nclient = Client(credentials=Credentials.from_env())\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprompt_name = \"My prompt\"\nmodel_id = \"google/flan-t5-xl\"\n\nprint(heading(\"Create prompt\"))\ntemplate = \"This is the recipe for {{meal}} as written by {{author}}: \"\ncreate_response = client.prompt.create(\n    model_id=model_id,\n    name=prompt_name,\n    input=template,\n    data={\"meal\": \"goulash\", \"author\": \"Shakespeare\"},\n    parameters=TextGenerationParameters(\n        length_penalty=LengthPenalty(decay_factor=1.5),\n        decoding_method=DecodingMethod.SAMPLE,\n    ),\n)\nprompt_id = create_response.result.id\nprint(f\"Prompt id: {prompt_id}\")\n\nprint(heading(\"Get prompt details\"))\nretrieve_response = client.prompt.retrieve(id=prompt_id)\npprint(retrieve_response.result.model_dump())\n\nprint(heading(\"Generate text using prompt\"))\nfor generation_response in client.text.generation.create(\n    prompt_id=prompt_id,\n    parameters=TextGenerationParameters(return_options=TextGenerationReturnOptions(input_text=True)),\n):\n    result = generation_response.results[0]\n    print(f\"Prompt: {result.input_text}\")\n    print(f\"Answer: {result.generated_text}\")\n\nprint(heading(\"Override prompt template variables\"))\nfor generation_response in client.text.generation.create(\n    prompt_id=prompt_id,\n    parameters=TextGenerationParameters(return_options=TextGenerationReturnOptions(input_text=True)),\n    data={\"meal\": \"pancakes\", \"author\": \"Edgar Allan Poe\"},\n):\n    result = generation_response.results[0]\n    print(f\"Prompt: {result.input_text}\")\n    print(f\"Answer: {result.generated_text}\")\n\n    print(heading(\"Show all existing prompts\"))\n    prompt_list_response = client.prompt.list(search=prompt_name, limit=10, offset=0)\n    print(\"Total Count: \", prompt_list_response.total_count)\n    print(\"Results: \", prompt_list_response.results)\n\n    print(heading(\"Delete prompt\"))\n    client.prompt.delete(id=prompt_id)\n    print(\"OK\")\n", "type": "example"}, {"source_path": "examples/request/request.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/request/request.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.request.request.html", "content": "\"\"\"\nWorking with your requests\n\nThe following example shows how to list your requests,\ndelete a request or a whole chat conversation (group of requests).\n\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import RequestEndpoint, RequestOrigin, RequestStatus\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"History of my requests\"))\nfor history_item in client.request.list(\n    limit=8,\n    offset=0,\n    status=RequestStatus.SUCCESS,\n    origin=RequestOrigin.API,\n    endpoint=RequestEndpoint.GENERATE,\n).results:\n    pprint(history_item.model_dump(include=[\"request\", \"created_at\", \"duration\"]))\n\n# Deletes a request\n# client.request.delete(request_id)\n\n# Deletes a whole chat conversation\n# client.request.chat_delete(conversation_id)\n", "type": "example"}, {"source_path": "examples/system_prompt/system_prompt.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/system_prompt/system_prompt.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.system_prompt.system_prompt.html", "content": "\"\"\"\nWorking with system prompts\n\nThe system prompt is a pre-defined prompt that helps cue the model to exhibit the desired behavior for a specific task.\n\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\nclient = Client(credentials=Credentials.from_env())\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nprint(heading(\"Create a system prompt\"))\nprompt_name = \"Simple Verbalizer\"\nprompt_content = \"\"\"classify { \"label 1\", \"label 2\" } Input: {{input}} Output:\"\"\"\ncreate_response = client.system_prompt.create(name=prompt_name, content=prompt_content)\nsystem_prompt_id = create_response.result.id\nprint(f\"System Prompt ID: {system_prompt_id}\")\n\nprint(heading(\"Get a system prompt details\"))\nretrieve_response = client.system_prompt.retrieve(id=system_prompt_id)\npprint(retrieve_response.result.model_dump())\n\nprint(heading(\"Show all existing system prompts\"))\nsystem_prompt_list_response = client.system_prompt.list(offset=0, limit=10)\nprint(\"Total Count: \", system_prompt_list_response.total_count)\nprint(\"Results: \", system_prompt_list_response.results)\n\nprint(heading(\"Delete a system prompt\"))\nclient.system_prompt.delete(id=system_prompt_id)\nprint(\"OK\")\n", "type": "example"}, {"source_path": "examples/text/chat.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/chat.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.chat.html", "content": "\"\"\"Chat with a model\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    HumanMessage,\n    ModerationHAP,\n    ModerationParameters,\n    SystemMessage,\n    TextGenerationParameters,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nparameters = TextGenerationParameters(\n    decoding_method=DecodingMethod.SAMPLE, max_new_tokens=128, min_new_tokens=30, temperature=0.7, top_k=50, top_p=1\n)\n\nclient = Client(credentials=Credentials.from_env())\nmodel_id = \"meta-llama/llama-2-70b-chat\"\n\nprompt = \"What is NLP and how it has evolved over the years?\"\nprint(heading(\"Generating a chat response\"))\nresponse = client.text.chat.create(\n    model_id=model_id,\n    messages=[\n        SystemMessage(\n            content=\"\"\"You are a helpful, respectful and honest assistant.\nAlways answer as helpfully as possible, while being safe.\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\nPlease ensure that your responses are socially unbiased and positive in nature. If a question does not make\nany sense, or is not factually coherent, explain why instead of answering something incorrectly.\nIf you don't know the answer to a question, please don't share false information.\n\"\"\",\n        ),\n        HumanMessage(content=prompt),\n    ],\n    parameters=parameters,\n)\nconversation_id = response.conversation_id\nprint(f\"Conversation ID: {conversation_id}\")\nprint(f\"Request: {prompt}\")\nprint(f\"Response: {response.results[0].generated_text}\")\n\nprint(heading(\"Continue with a conversation\"))\nprompt = \"How can I start?\"\nresponse = client.text.chat.create(\n    messages=[HumanMessage(content=prompt)],\n    moderations=ModerationParameters(hap=ModerationHAP(threshold=0.8)),\n    conversation_id=conversation_id,\n    use_conversation_parameters=True,\n)\nprint(f\"Request: {prompt}\")\nprint(f\"Response: {response.results[0].generated_text}\")\n", "type": "example"}, {"source_path": "examples/text/compare_parameters.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/compare_parameters.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.compare_parameters.html", "content": "\"\"\"\nCompare a set of hyperparameters\n\nRun a grid search over all possible combinations of parameters\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    TextGenerationComparisonCreateRequestRequest,\n    TextGenerationComparisonParameters,\n    TextGenerationParameters,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint> (optional) DEFAULT_API = \"https://bam-api.res.ibm.com\"\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprompt = \"The capital of Madrid is Spain. The capital of Canada is\"\n\nprint(heading(\"Run text generation with many parameter combinations\"))\nresponse = client.text.generation.compare(\n    request=TextGenerationComparisonCreateRequestRequest(\n        model_id=\"google/flan-t5-xxl\",\n        parameters=TextGenerationParameters(min_new_tokens=1, max_new_tokens=10, decoding_method=DecodingMethod.SAMPLE),\n        input=prompt,\n    ),\n    # Grid search through all possible combinations of the following parameters:\n    compare_parameters=TextGenerationComparisonParameters(\n        top_k=[10, 50],\n        repetition_penalty=[1.0, 1.5],\n        temperature=[0.7, 0.9, 1.5, 2.0],\n    ),\n)\n\nprint(f\"Prompt: {prompt}\\n\")\n\nfor params_combination in response.results:\n    print(f\"Used params: {params_combination.parameters.model_dump()}\")\n    assert params_combination.result\n    print(f\"Generated text: {params_combination.result.results[0].generated_text}\\n\")\n", "type": "example"}, {"source_path": "examples/text/embedding.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/embedding.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.embedding.html", "content": "\"\"\"Get embedding vectors for text data\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import TextEmbeddingParameters\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\ninputs = [\"Hello\", \"world\"]\nmodel_id = \"sentence-transformers/all-minilm-l6-v2\"\n\nprint(heading(\"Running embedding for inputs in parallel\"))\n# yields batch of results that are produced asynchronously and in parallel\nfor input, response in zip(\n    inputs,\n    client.text.embedding.create(\n        model_id=model_id,\n        inputs=inputs,\n        parameters=TextEmbeddingParameters(truncate_input_tokens=True),\n    ),\n):\n    print(f\"Embedding vector for '{input}': {response.results}\")\n", "type": "example"}, {"source_path": "examples/text/generation.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/generation.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.generation.html", "content": "\"\"\"\nComplex text generation example with a progress bar\n\nRuns text generation asynchronously in parallel to achieve better performance\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    LengthPenalty,\n    ModerationParameters,\n    ModerationStigma,\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n)\nfrom genai.text.generation import CreateExecutionOptions\n\ntry:\n    from tqdm.auto import tqdm\nexcept ImportError:\n    print(\"Please install tqdm to run this example.\")\n    raise\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\nclient = Client(credentials=Credentials.from_env())\n\ngreeting = \"Hello! How are you?\"\nlots_of_greetings = [greeting] * 20\n\nmoderations = ModerationParameters(\n    stigma=ModerationStigma(input=False, threshold=0.8),\n    # possibly add more moderations:\n    # implicit_hate=ModerationImplicitHate(...),\n    # hap=ModerationHAP(...),\n)\n\nparameters = TextGenerationParameters(\n    max_new_tokens=10,\n    decoding_method=DecodingMethod.SAMPLE,\n    length_penalty=LengthPenalty(start_index=5, decay_factor=1.5),\n    return_options=TextGenerationReturnOptions(\n        # if ordered is False, you can use return_options to retrieve the corresponding prompt\n        input_text=True,\n    ),\n)\n\nprint(heading(\"Generating responses in parallel\"))\n# yields batch of results that are produced asynchronously and in parallel\nfor idx, response in tqdm(\n    enumerate(\n        client.text.generation.create(\n            model_id=\"google/flan-t5-xl\",\n            inputs=lots_of_greetings,\n            moderations=moderations,\n            # set to ordered to True if you need results in the same order as prompts\n            execution_options=CreateExecutionOptions(ordered=False),\n            parameters=parameters,\n        )\n    ),\n    total=len(lots_of_greetings),\n    desc=\"Progress\",\n    unit=\"input\",\n):\n    result = response.results[0]\n    print(f\"Input text ({idx}): {result.input_text}\")\n    print(f\"Generated text ({idx}): {result.generated_text}\")\n", "type": "example"}, {"source_path": "examples/text/generation_streaming.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/generation_streaming.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.generation_streaming.html", "content": "\"\"\"Stream answer from a model\"\"\"\n\nimport sys\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    ModerationHAP,\n    ModerationParameters,\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n    TextModeration,\n)\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\n# Instantiate a model proxy object to send your requests\nclient = Client(credentials=Credentials.from_env())\n\n\nmodel_id = \"meta-llama/llama-2-70b\"\nprompt = \"The gesture of a hand with pinched fingers ü§å is actually rude in Italy. \"\nparameters = TextGenerationParameters(\n    decoding_method=DecodingMethod.SAMPLE,\n    max_new_tokens=90,\n    min_new_tokens=50,\n    return_options=TextGenerationReturnOptions(generated_tokens=True),\n    temperature=0.1,\n    repetition_penalty=1.5,\n    random_seed=3293482354,\n)\nmoderations = ModerationParameters(\n    hap=ModerationHAP(input=False, output=True, send_tokens=True, threshold=0.5),\n    # possibly add more moderations:\n    # implicit_hate=ModerationImplicitHate(...),\n    # stigma=ModerationStigma(...),\n)\nhate_speach_in_output: list[TextModeration] = []\n\n\nprint(heading(\"Generating text stream\"))\n\nprint(prompt, end=\"\")\nfor response in client.text.generation.create_stream(\n    model_id=model_id, input=prompt, parameters=parameters, moderations=moderations\n):\n    if not response.results:\n        hate_speach_in_output.extend(response.moderation.hap)\n        continue\n    for result in response.results:\n        if result.generated_text:\n            print(result.generated_text, end=\"\")\n\nprint()\nprint(heading(\"Hate speach in output\"), file=sys.stderr)\npprint([hap.model_dump() for hap in hate_speach_in_output], stream=sys.stderr)\n", "type": "example"}, {"source_path": "examples/text/moderation.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/moderation.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.moderation.html", "content": "\"\"\"\nModerate text data\n\nUncover HAP (hateful, abusive, profane language), Implicit hate or Stigma in text\n\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    HAPOptions,\n    ImplicitHateOptions,\n    StigmaOptions,\n)\nfrom genai.text.moderation import CreateExecutionOptions\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\ninputs = [\"Ice cream sucks!\", \"It tastes like poop.\"]\n\nprint(heading(\"Run text moderation in parallel\"))\n\nfor input_text, response in zip(\n    inputs,\n    client.text.moderation.create(\n        inputs=inputs,\n        hap=HAPOptions(threshold=0.5, send_tokens=True),\n        implicit_hate=ImplicitHateOptions(threshold=0.5, send_tokens=True),\n        stigma=StigmaOptions(threshold=0.5, send_tokens=True),\n        execution_options=CreateExecutionOptions(ordered=True),\n    ),\n):\n    print(f\"Input text: {input_text}\")\n    assert response.results\n    result = response.results[0]\n\n    # HAP\n    assert result.hap\n    hap = result.hap[0]\n    print(\"HAP:\")\n    pprint(hap.model_dump())\n\n    # Stigma\n    assert result.stigma\n    stigma = result.stigma[0]\n    print(\"Stigma:\")\n    pprint(stigma.model_dump())\n\n    # Implicit Hate\n    assert result.implicit_hate\n    implicit_hate = result.implicit_hate[0]\n    print(\"Implicit hate:\")\n    pprint(implicit_hate.model_dump())\n\n    print()\n", "type": "example"}, {"source_path": "examples/text/simple_generation.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/simple_generation.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.simple_generation.html", "content": "\"\"\"\nSimple text generation\n\"\"\"\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    TextGenerationParameters,\n    TextGenerationReturnOptions,\n)\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"Simple Text Generation\"))\n# yields batch of results that are produced asynchronously and in parallel\nfor response in client.text.generation.create(\n    model_id=\"google/flan-t5-xl\",\n    inputs=[\"What is a molecule?\", \"What is NLP?\"],\n    parameters=TextGenerationParameters(\n        max_new_tokens=150,\n        min_new_tokens=20,\n        return_options=TextGenerationReturnOptions(\n            input_text=True,\n        ),\n    ),\n):\n    result = response.results[0]\n    print(f\"Input Text: {result.input_text}\")\n    print(f\"Generated Text: {result.generated_text}\")\n    print(\"\")\n", "type": "example"}, {"source_path": "examples/text/tokenization.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/text/tokenization.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.text.tokenization.html", "content": "\"\"\"\nTokenize text data\n\nRuns tokenization asynchronously in parallel to achieve better performance\n\"\"\"\n\nfrom collections import Counter\nfrom datetime import datetime\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import TextTokenizationParameters, TextTokenizationReturnOptions\nfrom genai.text.tokenization import CreateExecutionOptions\n\ntry:\n    from tqdm.auto import tqdm\nexcept ImportError:\n    print(\"Please install tqdm to run this example.\")\n    raise\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\ngreeting = \"Hello! How are you?\"\nnum_of_greetings = 10_000\nlots_of_greetings = [greeting] * num_of_greetings\n\nbatch_size = 100\ntotal_tokens = 0\ntoken_frequency = Counter()\n\nstart_time = datetime.now()\n\nprint(heading(\"Running tokenization for many inputs in parallel\"))\n# yields batch of results that are produced asynchronously and in parallel\nfor response in tqdm(  # tqdm package can be used to show a progress bar during the tokenization\n    client.text.tokenization.create(\n        model_id=\"google/flan-t5-xl\",\n        input=lots_of_greetings,\n        execution_options=CreateExecutionOptions(\n            batch_size=batch_size,  # leave empty for optimal performance (specified for example purposes)\n            ordered=False,  # responses arrive in unspecified order if False, use True to match the order of inputs.\n        ),\n        parameters=TextTokenizationParameters(\n            return_options=TextTokenizationReturnOptions(\n                tokens=True,  # return tokens\n            )\n        ),\n    ),\n    total=num_of_greetings // batch_size,\n    unit=\"batch\",\n):\n    for result in response.results:\n        total_tokens += result.token_count\n        token_frequency.update(result.tokens)\n\nprint(f\"Total tokens: {total_tokens}\")\nprint(f\"Token frequency: {dict(token_frequency)}\")\nprint(f\"Time elapsed: {(datetime.now() - start_time).total_seconds():.2}s\")\n", "type": "example"}, {"source_path": "examples/tune/tune.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/tune/tune.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.tune.tune.html", "content": "\"\"\"\nTune a custom model (Prompt Tuning)\n\nUse custom training data to tune a model for text classification.\n\nNote:\n    This example has been written to enable an end-user to quickly try prompt-tuning. In order to obtain better\n    performance, a user would need to experiment with the number of observations and tuning hyperparameters\n\"\"\"\n\nimport time\nfrom pathlib import Path\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\nfrom genai.schema import (\n    DecodingMethod,\n    FilePurpose,\n    TextGenerationParameters,\n    TuneAssetType,\n    TuneParameters,\n    TuneStatus,\n    TuningType,\n)\n\nload_dotenv()\nnum_training_samples = 100\nnum_validation_samples = 20\ndata_root = Path(__file__).parent.resolve() / \".data\"\ntraining_file = data_root / \"fpb_train.jsonl\"\nvalidation_file = data_root / \"fpb_validation.jsonl\"\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\ndef create_dataset():\n    Path(data_root).mkdir(parents=True, exist_ok=True)\n    if training_file.exists() and validation_file.exists():\n        print(\"Dataset is already prepared\")\n        return\n\n    try:\n        import pandas as pd\n        from datasets import load_dataset\n    except ImportError:\n        print(\"Please install datasets and pandas for downloading the dataset.\")\n        raise\n\n    data = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n    df = pd.DataFrame(data[\"train\"]).sample(n=num_training_samples + num_validation_samples)\n    df.rename(columns={\"sentence\": \"input\", \"label\": \"output\"}, inplace=True)\n    df[\"output\"] = df[\"output\"].astype(str)\n    train_jsonl = df.iloc[:num_training_samples].to_json(orient=\"records\", lines=True, force_ascii=True)\n    validation_jsonl = df.iloc[num_training_samples:].to_json(orient=\"records\", lines=True, force_ascii=True)\n    with open(training_file, \"w\") as fout:\n        fout.write(train_jsonl)\n    with open(validation_file, \"w\") as fout:\n        fout.write(validation_jsonl)\n\n\ndef upload_files(client: Client, update=True):\n    files_info = client.file.list(search=training_file.name).results\n    files_info += client.file.list(search=validation_file.name).results\n\n    filenames_to_id = {f.file_name: f.id for f in files_info}\n    for filepath in [training_file, validation_file]:\n        filename = filepath.name\n        if filename in filenames_to_id and update:\n            print(f\"File already present: Overwriting {filename}\")\n            client.file.delete(filenames_to_id[filename])\n            response = client.file.create(file_path=filepath, purpose=FilePurpose.TUNE)\n            filenames_to_id[filename] = response.result.id\n        if filename not in filenames_to_id:\n            print(f\"File not present: Uploading {filename}\")\n            response = client.file.create(file_path=filepath, purpose=FilePurpose.TUNE)\n            filenames_to_id[filename] = response.result.id\n    return filenames_to_id[training_file.name], filenames_to_id[validation_file.name]\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"Creating dataset\"))\ncreate_dataset()\n\nprint(heading(\"Uploading files\"))\ntraining_file_id, validation_file_id = upload_files(client, update=True)\n\nhyperparams = TuneParameters(num_epochs=2, verbalizer='classify { \"0\", \"1\", \"2\" } Input: {{input}} Output:')\n\nprint(heading(\"Tuning model\"))\ntune_result = client.tune.create(\n    model_id=\"google/flan-t5-xl\",\n    name=\"classification-mpt-tune-api\",\n    tuning_type=TuningType.PROMPT_TUNING,\n    task_id=\"classification\",  # Another supported task is \"summarization\"\n    parameters=hyperparams,\n    training_file_ids=[training_file_id],\n    # validation_file_ids=[validation_file_id], # TODO: Broken at the moment - this causes tune to fail\n).result\n\nwhile tune_result.status not in [TuneStatus.FAILED, TuneStatus.HALTED, TuneStatus.COMPLETED]:\n    new_tune_result = client.tune.retrieve(tune_result.id).result\n    print(f\"Waiting for tune to finish, current status: {tune_result.status}\")\n    tune_result = new_tune_result\n    time.sleep(10)\n\nif tune_result.status in [TuneStatus.FAILED, TuneStatus.HALTED]:\n    print(\"Model tuning failed or halted\")\n    exit(1)\n\nprint(heading(\"Classify with a tuned model\"))\nprompt = \"Return on investment was 5.0 % , compared to a negative 4.1 % in 2009 .\"\nprint(\"Prompt: \", prompt)\ngen_params = TextGenerationParameters(decoding_method=DecodingMethod.SAMPLE, max_new_tokens=1, min_new_tokens=1)\ngen_response = next(client.text.generation.create(model_id=tune_result.id, inputs=[prompt]))\nprint(\"Answer: \", gen_response.results[0].generated_text)\n\nprint(heading(\"Get list of tuned models\"))\ninteresting_metadata_fields = [\"name\", \"id\", \"model_id\", \"created_at\", \"status\"]\ntune_list = client.tune.list(limit=5, offset=0)\nfor tune in tune_list.results:\n    pprint(tune.model_dump(include=interesting_metadata_fields))\n\n\nprint(heading(\"Retrieve metadata for a single tune\"))\ntune_detail = client.tune.retrieve(id=tune_result.id).result\npprint(tune_detail.model_dump(include=interesting_metadata_fields + [\"parameters\"]))\n\nprint(heading(\"Downloading tune model assets\"))\nlogs = client.tune.read(id=tune_result.id, type=TuneAssetType.LOGS).decode(\"utf-8\")\nprint(logs)\n\nprint(heading(\"Deleting a tuned model\"))\nclient.tune.delete(tune_result.id)\nprint(\"OK\")\n", "type": "example"}, {"source_path": "examples/user/user.py", "source_url": "https://raw.githubusercontent.com/IBM/ibm-generative-ai/main/examples/user/user.py", "documentation_url": "https://ibm.github.io/ibm-generative-ai/main/rst_source/examples.user.user.html", "content": "\"\"\"Show information about current user\"\"\"\n\nfrom pprint import pprint\n\nfrom dotenv import load_dotenv\n\nfrom genai.client import Client\nfrom genai.credentials import Credentials\n\n# make sure you have a .env file under genai root with\n# GENAI_KEY=<your-genai-key>\n# GENAI_API=<genai-api-endpoint>\nload_dotenv()\n\n\ndef heading(text: str) -> str:\n    \"\"\"Helper function for centering text.\"\"\"\n    return \"\\n\" + f\" {text} \".center(80, \"=\") + \"\\n\"\n\n\nclient = Client(credentials=Credentials.from_env())\n\nprint(heading(\"Get user info\"))\nuser_info = client.user.retrieve().result\npprint(user_info.model_dump())\n\nprint(heading(\"Accept terms of use and give data usage consent\"))\nuser_update_info = client.user.update(tou_accepted=True).result\npprint(user_update_info.model_dump())\n", "type": "example"}]